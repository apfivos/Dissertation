{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d7bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, numpy as np, pandas as pd\n",
    "import os, gc, numpy as np, pandas as pd, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.sandwich_covariance import cov_hac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eced18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab libares\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd6fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the seeds theat they used and we keep constant for reproduction\n",
    "seeds=[3,154295,583240,321536,0, 865466, 260937, 32, 23549, 469686]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab6fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE               = 'FULL'      \n",
    "XZ_PATH            = '/content/drive/MyDrive/Dissertation/data/Xz_float32.npy'   #from the outputs of the data_cleaning_and_pca file\n",
    "META_PATH          = '/content/drive/MyDrive/Dissertation/data/meta.parquet'     #from the outputs of the data_cleaning_and_pca file\n",
    "TARGET_COL         = 'r_dh_no_tc' #or  'r_dh_tc'\n",
    "BATCH_SIZE         = 8192\n",
    "LR                 = 1e-3\n",
    "WEIGHT_DECAY       = 1e-5\n",
    "DROPOUT_P          = 0.4\n",
    "N_EPOCHS_INIT      = 12          \n",
    "N_EPOCHS_PER_YEAR  = 6           \n",
    "SEEDS              = seeds   \n",
    "ENSEMBLE_N         = len(SEEDS)        \n",
    "DEVICE             = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "PORTS_N            = 10           #deciles\n",
    "LAGS_NW            = 6          \n",
    "SAVE_DIR           = None         \n",
    "ADD_MASK_CHANNEL   = False\n",
    "GRAD_CLIP_NORM     = 5.0\n",
    "DISPLAY_IN_PCT     = True        \n",
    "PRE_STANDARDIZED_INPUT = True     \n",
    "\n",
    "def _require_cols(df, cols):\n",
    "    miss = [c for c in cols if c not in df.columns]\n",
    "    if miss: raise ValueError(f\"no columns in meta: {miss}\")\n",
    "\n",
    "def _ensure_channel_dim(X):\n",
    "    X = np.asarray(X)\n",
    "    if X.ndim == 3:\n",
    "        X = X[:, None, :, :]\n",
    "    elif X.ndim == 4 and X.shape[1] == 1:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"Xz should be (N,H,W) or (N,1,H,W).it is {X.shape}\")\n",
    "    return np.ascontiguousarray(X.astype(np.float32))\n",
    "\n",
    "def _load_Xz(path):\n",
    "    if path is None:\n",
    "        if 'Xz' not in globals():\n",
    "            raise RuntimeError(\"Xz not in ram or XZ_PATH=None.\")\n",
    "        return _ensure_channel_dim(globals()['Xz'])\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext == '.npy':\n",
    "        X = np.load(path, allow_pickle=False)\n",
    "    elif ext == '.npz':\n",
    "        npz = np.load(path, allow_pickle=False)\n",
    "        X = npz['Xz'] if 'Xz' in npz.files else npz[npz.files[0]]\n",
    "    else:\n",
    "        raise ValueError(\"XZ_PATH should be .npy or .npz\")\n",
    "    return _ensure_channel_dim(X)\n",
    "\n",
    "def _load_meta(path):\n",
    "    if path is None:\n",
    "        if 'meta' not in globals():\n",
    "            raise RuntimeError(\"Meta not in ram or META_PATH=None.\")\n",
    "        return globals()['meta'].copy()\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext == '.parquet':\n",
    "        df = pd.read_parquet(path)\n",
    "    elif ext == '.csv':\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        raise ValueError(\"META_PATH should be  .parquet or .csv\")\n",
    "    return df\n",
    "\n",
    "#standardization if not already\n",
    "def compute_pixel_stats(X, idx):\n",
    "    Xi = X[idx].astype(np.float32)\n",
    "    Xi = np.where(np.isfinite(Xi), Xi, np.nan)\n",
    "    mu = np.nanmean(Xi, axis=0)\n",
    "    sd = np.nanstd(Xi, axis=0, ddof=0)\n",
    "    sd = np.where((~np.isfinite(sd)) | (sd < 1e-8), 1.0, sd)\n",
    "    mu = np.where(~np.isfinite(mu), 0.0, mu)\n",
    "    return mu.astype(np.float32), sd.astype(np.float32)\n",
    "\n",
    "def standardize_and_stack(X, mu, sd, mask=None, add_mask_channel=False):\n",
    "    \"\"\"\n",
    "    IF mu=0 & sd=1 then the transfromation has no impact \n",
    "    \"\"\"\n",
    "    X = X.astype(np.float32)\n",
    "    Xf = np.where(np.isfinite(X), X, np.nan)\n",
    "    Z  = (Xf - mu) / sd\n",
    "    Z  = np.where(np.isfinite(Z), Z, 0.0)\n",
    "    if add_mask_channel:\n",
    "        if mask is None:\n",
    "            mask = np.isfinite(X).astype(np.float32)\n",
    "        Z = np.concatenate([Z, mask.astype(np.float32)], axis=1)  # (N,2,H,W)\n",
    "    return np.ascontiguousarray(Z.astype(np.float32))\n",
    "\n",
    "class SurfaceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = np.ascontiguousarray(X.astype(np.float32))\n",
    "        self.y = np.asarray(y, dtype=np.float32)\n",
    "    def __len__(self): return self.y.shape[0]\n",
    "    def __getitem__(self, k):\n",
    "        return torch.from_numpy(self.X[k]), torch.tensor(self.y[k], dtype=torch.float32)\n",
    "    \n",
    "class HoflerCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    4 conv blocks: (16,32,64,128), 3x3, LeakyReLU, BatchNorm.\n",
    "    MaxPool 2x2 at 3 first blocks, Global Avg Pool, Dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=1, p_drop=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, 16, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64,128, kernel_size=3, padding=1)\n",
    "        self.bn4   = nn.BatchNorm2d(128)\n",
    "        self.act   = nn.LeakyReLU(0.1, inplace=True)\n",
    "        self.pool  = nn.MaxPool2d(2,2)\n",
    "        self.drop  = nn.Dropout(p_drop)\n",
    "        self.out   = nn.Linear(128, 1)\n",
    "        self._init_weights()\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.act(self.bn1(self.conv1(x))))  # 10×18 → 5×9\n",
    "        x = self.pool(self.act(self.bn2(self.conv2(x))))  # 5×9 → 2×4\n",
    "        x = self.pool(self.act(self.bn3(self.conv3(x))))  # 2×4 → 1×2\n",
    "        x = self.act(self.bn4(self.conv4(x)))             # 1×2\n",
    "        x = F.adaptive_avg_pool2d(x, (1,1))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.drop(x)\n",
    "        x = self.out(x).squeeze(1)\n",
    "        return x\n",
    "\n",
    "#train and predicitons\n",
    "def make_loader(X, y, bs=BATCH_SIZE, shuffle=True):\n",
    "    return DataLoader(SurfaceDataset(X, y),\n",
    "                      batch_size=bs, shuffle=shuffle,\n",
    "                      num_workers=8, pin_memory=(DEVICE=='cuda'))\n",
    "\n",
    "def train_epochs(model, loader, nepochs, opt=None, device=DEVICE):\n",
    "    model.to(device); model.train()\n",
    "    if opt is None:\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    for ep in range(1, nepochs+1):\n",
    "        losses = []\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device, non_blocking=True); yb = yb.to(device, non_blocking=True)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            pred = model(xb).float()\n",
    "            loss = F.mse_loss(pred, yb)\n",
    "            if not torch.isfinite(loss):\n",
    "                fin_x = float(torch.isfinite(xb).float().mean().item())\n",
    "                fin_y = float(torch.isfinite(yb).float().mean().item())\n",
    "                raise RuntimeError(f\"NaN/Inf loss. finite_ratio(x)={fin_x:.3f}, finite_ratio(y)={fin_y:.3f}\")\n",
    "            loss.backward()\n",
    "            if GRAD_CLIP_NORM is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)\n",
    "            opt.step()\n",
    "            losses.append(loss.item())\n",
    "        tqdm.write(f\"  epoch {ep}/{nepochs}  train MSE={np.mean(losses):.6f}\")\n",
    "    return opt\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, X, device=DEVICE, bs=8192):\n",
    "    model.eval(); model.to(device)\n",
    "    out = np.empty(X.shape[0], dtype=np.float32)\n",
    "    for i in range(0, X.shape[0], bs):\n",
    "        xb = torch.from_numpy(X[i:i+bs]).to(device)\n",
    "        out[i:i+bs] = model(xb).float().cpu().numpy()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def newey_west_mean_t(y: pd.Series, lags: int = 6):\n",
    "    y = pd.Series(y).astype(float).dropna()\n",
    "    if y.empty: return np.nan, np.nan\n",
    "    X = np.ones((len(y),1))\n",
    "    res = sm.OLS(y.values, X).fit()\n",
    "    V = cov_hac(res, nlags=lags)\n",
    "    mu = float(res.params[0])\n",
    "    se = float(np.sqrt(V[0,0])) if (V is not None and V[0,0]>0) else np.nan\n",
    "    t  = mu/se if (se is not None and se>0) else np.nan\n",
    "    return mu, t\n",
    "\n",
    "def sharpe_ann(y: pd.Series):\n",
    "    y = pd.Series(y).astype(float).dropna()\n",
    "    s = y.std(ddof=1)\n",
    "    return (y.mean()/s)*np.sqrt(12.0) if s>0 else np.nan\n",
    "\n",
    "#rank-based assignment \n",
    "def _assign_ports_per_month_rank(s: pd.Series, n: int) -> pd.Series:\n",
    "    x = pd.Series(s)\n",
    "    mask = x.notna(); xn = x[mask]\n",
    "    k = min(n, int(xn.nunique()))\n",
    "    if (k < 2) or (len(xn) < 2):\n",
    "        return pd.Series(pd.array([pd.NA]*len(x), dtype=\"Int64\"), index=x.index)\n",
    "    r = xn.rank(method='first')\n",
    "    nobs = len(xn)\n",
    "    port = ((r - 1) * k / nobs).astype(int) + 1\n",
    "    port = port.clip(1, k).astype(\"Int64\")\n",
    "    out = pd.Series(pd.array([pd.NA]*len(x), dtype=\"Int64\"), index=x.index)\n",
    "    out.loc[xn.index] = port\n",
    "    return out\n",
    "\n",
    "def _monthly_port_avgs(d, weight_col, pred_col, ret_col):\n",
    "    if weight_col is None:\n",
    "        g = (d.groupby(['month','port'], observed=True)\n",
    "               .agg(pred=(pred_col,'mean'), real=(ret_col,'mean'))\n",
    "               .reset_index())\n",
    "    else:\n",
    "        grp = d[['month','port', pred_col, ret_col, weight_col]].dropna()\n",
    "        grp = grp[grp[weight_col] > 0]\n",
    "        for col in (pred_col, ret_col):\n",
    "            grp[f'w_{col}'] = grp[col]*grp[weight_col]\n",
    "        g = (grp.groupby(['month','port'], observed=True)\n",
    "                 .agg(pred=('w_'+pred_col,'sum'),\n",
    "                      real=('w_'+ret_col,'sum'),\n",
    "                      w=(weight_col,'sum'))\n",
    "                 .reset_index())\n",
    "        g['pred'] = np.where(g['w']>0, g['pred']/g['w'], np.nan)\n",
    "        g['real'] = np.where(g['w']>0, g['real']/g['w'], np.nan)\n",
    "        g = g.drop(columns='w')\n",
    "    return g\n",
    "\n",
    "def cnn_decile_table(meta, pred_col, ret_col, n_ports=10, weighting='oiw', lags=LAGS_NW):\n",
    "    d = meta.copy()\n",
    "    _require_cols(d, ['month', pred_col, ret_col])\n",
    "    if 'doi' not in d.columns: d['doi'] = 1.0\n",
    "    d = d.dropna(subset=['month', pred_col, ret_col])\n",
    "    d['month'] = pd.to_datetime(d['month'])\n",
    "    d['port'] = d.groupby('month', observed=True)[pred_col].transform(\n",
    "                  lambda s: _assign_ports_per_month_rank(s, n_ports)\n",
    "               )\n",
    "    d = d.dropna(subset=['port'])\n",
    "    d['port'] = d['port'].astype('Int64')\n",
    "\n",
    "    weight_col = None if weighting.lower()=='ew' else 'doi'\n",
    "    g = _monthly_port_avgs(d, weight_col, pred_col, ret_col) \n",
    "\n",
    "    maxp = d.groupby('month', observed=True)['port'].max().rename('maxp')\n",
    "    gj   = g.merge(maxp, on='month', how='left')\n",
    "\n",
    "    #time series\n",
    "    ts_pred = {}\n",
    "    ts_real = {}\n",
    "    for k in range(1, n_ports+1):\n",
    "        ts_pred[k] = g[g['port']==k].set_index('month')['pred'].sort_index()\n",
    "        ts_real[k] = g[g['port']==k].set_index('month')['real'].sort_index()\n",
    "\n",
    "    #Low/High per month\n",
    "    low_pred_ts  = g[g['port']==1].set_index('month')['pred'].sort_index()\n",
    "    low_real_ts  = g[g['port']==1].set_index('month')['real'].sort_index()\n",
    "    high_pred_ts = gj[gj['port']==gj['maxp']].set_index('month')['pred'].sort_index()\n",
    "    high_real_ts = gj[gj['port']==gj['maxp']].set_index('month')['real'].sort_index()\n",
    "\n",
    "    #HML time series\n",
    "    hml_pred_ts = (high_pred_ts - low_pred_ts).dropna()\n",
    "    hml_real_ts = (high_real_ts - low_real_ts).dropna()\n",
    "\n",
    "    rows = []\n",
    "    labels = (['Low'] + list(range(2, n_ports)) + ['High'])\n",
    "    for lbl in labels:\n",
    "        if lbl == 'Low':\n",
    "            pr, rr = low_pred_ts, low_real_ts\n",
    "        elif lbl == 'High':\n",
    "            pr, rr = high_pred_ts, high_real_ts\n",
    "        else:\n",
    "            k = int(lbl)\n",
    "            pr, rr = ts_pred.get(k, pd.Series(dtype=float)), ts_real.get(k, pd.Series(dtype=float))\n",
    "        mu_pred = float(pd.Series(pr).dropna().mean()) if len(pr)>0 else np.nan\n",
    "        mu_real = float(pd.Series(rr).dropna().mean()) if len(rr)>0 else np.nan\n",
    "        tstat   = newey_west_mean_t(rr, lags=lags)[1]\n",
    "        sr      = sharpe_ann(rr)\n",
    "        rows.append({'Bucket': lbl, 'Pred.': mu_pred, 'Real.': mu_real, 't': tstat, 'SR': sr})\n",
    "\n",
    "    #HML row\n",
    "    mu_pred_hml = float(hml_pred_ts.mean()) if not hml_pred_ts.empty else np.nan\n",
    "    mu_real_hml = float(hml_real_ts.mean()) if not hml_real_ts.empty else np.nan\n",
    "    t_hml       = newey_west_mean_t(hml_real_ts, lags=lags)[1]\n",
    "    sr_hml      = sharpe_ann(hml_real_ts)\n",
    "    rows.append({'Bucket': 'HML', 'Pred.': mu_pred_hml, 'Real.': mu_real_hml, 't': t_hml, 'SR': sr_hml})\n",
    "\n",
    "    tbl = pd.DataFrame(rows).set_index('Bucket')\n",
    "    return tbl, {'HML': hml_real_ts}\n",
    "\n",
    "\n",
    "def main():\n",
    "    X = _load_Xz(XZ_PATH)             \n",
    "    df = _load_meta(META_PATH).copy()\n",
    "    _require_cols(df, ['month', TARGET_COL])\n",
    "    if 'doi' not in df.columns: df['doi'] = 1.0\n",
    "    df['month'] = pd.to_datetime(df['month'])\n",
    "    assert len(df) == X.shape[0], f\"Not correct amount of lines at len(meta)={len(df)} vs N in Xz={X.shape[0]}\"\n",
    "\n",
    "    finite_ratio_X = float(np.isfinite(X).mean())\n",
    "    finite_ratio_y = float(np.isfinite(df[TARGET_COL].to_numpy()).mean())\n",
    "    pix_nan_rate = (~np.isfinite(X)).mean(axis=0)  \n",
    "    print(f\"Xz finite ratio: {finite_ratio_X:.4f} | Target finite ratio: {finite_ratio_y:.4f}\")\n",
    "    print(\"Pixel NaN rate [min/mean/max]:\",\n",
    "          float(pix_nan_rate.min()), float(pix_nan_rate.mean()), float(pix_nan_rate.max()))\n",
    "\n",
    "    # 3) Define months & OOS start (min 7y)\n",
    "    months = np.sort(df['month'].unique())\n",
    "    min_month = pd.to_datetime(df['month'].min())\n",
    "    first_oos_month = min_month + pd.DateOffset(years=7)\n",
    "    if first_oos_month > months.max():\n",
    "        raise RuntimeError(\"Not enough data for 7 years training\")\n",
    "\n",
    "    #fine tuning\n",
    "    years_oos = sorted(pd.Series(months[months >= first_oos_month]).dt.year.unique())\n",
    "    if MODE.upper() == 'FAST' and len(years_oos) > 2:\n",
    "        years_oos = years_oos[-2:]  # smoke test δύο χρόνια\n",
    "\n",
    "    print(f\"Mode={MODE} | OOS years: {len(years_oos)} | Xz shape: {X.shape} | meta shape: {df.shape}\")\n",
    "\n",
    "    IN_CH = 1 + (1 if ADD_MASK_CHANNEL else 0)\n",
    "    MASK = np.isfinite(X).astype(np.float32) if ADD_MASK_CHANNEL else None\n",
    "\n",
    "    if PRE_STANDARDIZED_INPUT:\n",
    "        mu = np.zeros(X.shape[1:], dtype=np.float32)   \n",
    "        sd = np.ones (X.shape[1:], dtype=np.float32)   \n",
    "        print(\"PRE_STANDARDIZED_INPUT=True:identity transform.\")\n",
    "    else:\n",
    "        init_idx = np.where(df['month'].lt(first_oos_month).values)[0]\n",
    "        mu, sd = compute_pixel_stats(X, init_idx)\n",
    "\n",
    "    rb_hat = np.full(len(df), np.nan, dtype=np.float32)\n",
    "    models, opts = {}, {}\n",
    "    for s in SEEDS:\n",
    "        torch.manual_seed(s); np.random.seed(s)\n",
    "        m = HoflerCNN(in_ch=IN_CH, p_drop=DROPOUT_P)\n",
    "        o = torch.optim.Adam(m.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "        models[s], opts[s] = m, o\n",
    "\n",
    "    def make_tr_loader(idx):\n",
    "        X_tr = standardize_and_stack(X[idx], mu, sd,\n",
    "                                     mask=None if MASK is None else MASK[idx],\n",
    "                                     add_mask_channel=ADD_MASK_CHANNEL)\n",
    "        y_tr = df.loc[idx, TARGET_COL].to_numpy(np.float32)\n",
    "        good = np.isfinite(y_tr)\n",
    "        if (~good).sum() > 0:\n",
    "            print(f\"Warning: βρέθηκαν {(~good).sum()} μη-περατά targets — αγνοούνται στο training.\")\n",
    "        return make_loader(X_tr[good], y_tr[good], bs=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    #annual walk-forward\n",
    "    for i, Y in enumerate(tqdm(years_oos, desc=\"OOS years\")):\n",
    "        start_Y = pd.Timestamp(year=Y, month=1, day=1)\n",
    "        start_next = pd.Timestamp(year=Y+1, month=1, day=1)\n",
    "\n",
    "        tr_idx = np.where(df['month'].lt(start_Y).values)[0]\n",
    "        te_mask = (df['month'] >= start_Y) & (df['month'] < start_next) & (df['month'] >= first_oos_month)\n",
    "        te_idx  = np.where(te_mask.values)[0]\n",
    "        if len(tr_idx)==0 or len(te_idx)==0:\n",
    "            continue\n",
    "        tr_loader = make_tr_loader(tr_idx)\n",
    "        ne = N_EPOCHS_INIT if i==0 else N_EPOCHS_PER_YEAR\n",
    "        print(f\"\\n=== Year {Y} | train_n={len(tr_idx):,} | test_n={len(te_idx):,} | epochs={ne} ===\")\n",
    "        oos_preds = []\n",
    "        for s in SEEDS:\n",
    "            opts[s] = train_epochs(models[s], tr_loader, nepochs=ne, opt=opts[s], device=DEVICE)\n",
    "            X_te = standardize_and_stack(X[te_idx], mu, sd,\n",
    "                                         mask=None if MASK is None else MASK[te_idx],\n",
    "                                         add_mask_channel=ADD_MASK_CHANNEL)\n",
    "            oos_preds.append(predict(models[s], X_te, device=DEVICE))\n",
    "        rb_hat[te_idx] = np.mean(np.vstack(oos_preds), axis=0)\n",
    "        del tr_loader; gc.collect()\n",
    "        if DEVICE=='cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    #Collect OOS predictions\n",
    "    meta_eval = df.copy()\n",
    "    meta_eval['rb_cnn'] = rb_hat\n",
    "    nnz = int(np.isfinite(meta_eval['rb_cnn']).sum())\n",
    "    print(f\"\\nOOS non-NaN preds: {nnz} / {len(meta_eval)}\")\n",
    "    if nnz == 0:\n",
    "        raise RuntimeError(\"no predictions\")\n",
    "\n",
    "    #tables\n",
    "    print(f\"\\nBuilding decile tables (N={PORTS_N}) ...\")\n",
    "    tbl_oiw, hml_ts_oiw = cnn_decile_table(meta_eval, 'rb_cnn', TARGET_COL, n_ports=PORTS_N, weighting='oiw', lags=LAGS_NW)\n",
    "    tbl_ew,  hml_ts_ew  = cnn_decile_table(meta_eval, 'rb_cnn', TARGET_COL, n_ports=PORTS_N, weighting='ew',  lags=LAGS_NW)\n",
    "\n",
    "    def _fmt(tbl):\n",
    "        out = tbl.copy()\n",
    "        if DISPLAY_IN_PCT:\n",
    "            for c in ['Pred.','Real.']:\n",
    "                out[c] = 100.0 * out[c]\n",
    "        return out\n",
    "\n",
    "    print(\"\\n=== (a) Dollar open interest weighted ===\")\n",
    "    print(_fmt(tbl_oiw).round({'Pred.':2,'Real.':2,'t':2,'SR':2}).to_string())\n",
    "\n",
    "    print(\"\\n=== (b) Equal weighted ===\")\n",
    "    print(_fmt(tbl_ew).round({'Pred.':2,'Real.':2,'t':2,'SR':2}).to_string())\n",
    "\n",
    "    #saving\n",
    "    if SAVE_DIR:\n",
    "        os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "        meta_eval.to_parquet(os.path.join(SAVE_DIR, 'meta_eval_with_rb_cnn.parquet'))\n",
    "        tbl_oiw.to_csv(os.path.join(SAVE_DIR, 'cnn_table_oiw.csv'))\n",
    "        tbl_ew.to_csv(os.path.join(SAVE_DIR, 'cnn_table_ew.csv'))\n",
    "        hml_ts_oiw['HML'].to_csv(os.path.join(SAVE_DIR, 'hml_oiw_series.csv'))\n",
    "        hml_ts_ew['HML'].to_csv(os.path.join(SAVE_DIR, 'hml_ew_series.csv'))\n",
    "        print(f\"\\nSaved outputs to: {os.path.abspath(SAVE_DIR)}\")\n",
    "\n",
    "    return meta_eval, tbl_oiw, tbl_ew\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _ = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06423dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, gc, math, numpy as np, pandas as pd\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from torchdiffeq import odeint\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.sandwich_covariance import cov_hac\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bfc8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    MODE               = 'FULL'             \n",
    "    XZ_PATH            = '/content/drive/MyDrive/Dissertation/data/Xz_float32.npy' #same as before at the cnn\n",
    "    META_PATH          = '/content/drive/MyDrive/Dissertation/data/meta.parquet'\n",
    "    TARGET_COL         = 'r_dh_no_tc'\n",
    "    PRE_STANDARDIZED_INPUT = True           \n",
    "    ADD_MASK_CHANNEL   = False             \n",
    "    DISPLAY_IN_PCT     = True\n",
    "\n",
    "    DEVICE             = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    AMP_DTYPE          = torch.bfloat16     \n",
    "    USE_SCALER         = False              \n",
    "    CHANNELS_LAST      = True               \n",
    "    TORCH_COMPILE      = False              \n",
    "\n",
    "    BATCH_SIZE         = 8192\n",
    "    LR                 = 1e-3\n",
    "    WEIGHT_DECAY       = 1e-5\n",
    "    GRAD_CLIP_NORM     = 5.0\n",
    "    DROPOUT_P          = 0.4\n",
    "\n",
    "    EPOCHS_INIT        = 12                \n",
    "    EPOCHS_PER_YEAR    = 6                  \n",
    "    SEEDS              = seeds             \n",
    "\n",
    "    #NODE\n",
    "    DZ                 = 128\n",
    "    VF_WIDTH           = 256\n",
    "    ODE_METHOD         = 'dopri5'\n",
    "    ODE_RTOL           = 1e-3\n",
    "    ODE_ATOL           = 1e-4\n",
    "    ODE_MAX_STEPS      = 64\n",
    "    ADJOINT            = False\n",
    "\n",
    "\n",
    "    LOSS_ALPHA_MSE     = 0.75\n",
    "\n",
    "    PORTS_N            = 10\n",
    "    LAGS_NW            = 6\n",
    "\n",
    "\n",
    "    OPTION_CHAR_COLS   = ['spread','delta','gamma','vega','theta','iv','odp']\n",
    "    OPTION_CHAR_PCT    = ['spread']        \n",
    "    HOF_ROUND          = 2\n",
    "    OPTION_CHAR_SCALERS = {\n",
    "        'gamma': 50.0,   \n",
    "        'vega' : 0.01,   \n",
    "        'theta': 0.02,  \n",
    "\n",
    "    }\n",
    "\n",
    "    #SHAP\n",
    "    SHAP_ENABLE            = True\n",
    "    SHAP_BG_SAMPLES        = 64\n",
    "    SHAP_EXPLAIN_SAMPLES   = 64\n",
    "    SHAP_SAVE_DIR          = './shap_out'\n",
    "    SHAP_SHOW_FIGS        = True  \n",
    "    SHAP_SAVE_FIGS        = True   \n",
    "    SHAP_SAVE_STATS       = False  \n",
    "    SHAP_STATS_TOPK       = 10     \n",
    "\n",
    "    #LIME\n",
    "    LIME_ENABLE            = True\n",
    "    LIME_SAVE_DIR          = './lime_out'\n",
    "    LIME_NUM_SAMPLES       = 1500     \n",
    "    LIME_NUM_FEATURES      = 12        \n",
    "    LIME_SEGMENTATION_MODE = 'grid'   \n",
    "    LIME_SLIC_SEGMENTS     = 100       \n",
    "    LIME_BATCH_SIZE        = 2048      \n",
    "    LIME_SHOW_FIGS = True\n",
    "    LIME_SAVE_FIGS = True\n",
    "\n",
    "\n",
    "\n",
    "def _ensure_channel_dim(X):\n",
    "    X = np.asarray(X, dtype=np.float32)\n",
    "    if X.ndim == 3:  #(N,H,W)\n",
    "        X = X[:, None, :, :]\n",
    "    if not (X.ndim == 4 and X.shape[1] in (1,2)):\n",
    "        raise ValueError(f\"Xz must be (N,1,H,W) or (N,2,H,W). Got: {X.shape}\")\n",
    "    return np.ascontiguousarray(X)\n",
    "\n",
    "def load_Xz(path):\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext == '.npy':\n",
    "        X = np.load(path, allow_pickle=False)\n",
    "    elif ext == '.npz':\n",
    "        npz = np.load(path, allow_pickle=False)\n",
    "        X = npz['Xz'] if 'Xz' in npz.files else npz[npz.files[0]]\n",
    "    else:\n",
    "        raise ValueError(\"XZ_PATH should be .npy or .npz\")\n",
    "    return _ensure_channel_dim(X)\n",
    "\n",
    "def load_meta(path):\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext == '.parquet':\n",
    "        df = pd.read_parquet(path)\n",
    "    elif ext == '.csv':\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        raise ValueError(\"META_PATH should be .parquet or .csv\")\n",
    "    return df\n",
    "\n",
    "def identity_stack(X, add_mask=False):\n",
    "    X = np.asarray(X, dtype=np.float32)\n",
    "    Xf = np.where(np.isfinite(X), X, 0.0)\n",
    "    if add_mask:\n",
    "        mask = np.isfinite(X).astype(np.float32)\n",
    "        Xf = np.concatenate([Xf, mask], axis=1)\n",
    "    return np.ascontiguousarray(Xf)\n",
    "\n",
    "class SurfDS(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = np.ascontiguousarray(X, dtype=np.float32)  #(N,C,H,W)\n",
    "        self.y = np.asarray(y, dtype=np.float32)\n",
    "    def __len__(self): return self.y.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return torch.from_numpy(self.X[i]), torch.tensor(self.y[i], dtype=torch.float32)\n",
    "\n",
    "def make_loader(X, y, bs, shuffle=True, device=CFG.DEVICE):\n",
    "    return DataLoader(SurfDS(X,y), batch_size=bs, shuffle=shuffle,\n",
    "                      num_workers=0, pin_memory=(device=='cuda'), drop_last=False)\n",
    "\n",
    "#CNN->NODE\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Höfler-style 4 conv blocks + GAP + dropout -> 128-dim.\"\"\"\n",
    "    def __init__(self, in_ch=1, p_drop=0.35):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, 16, 3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64,128, 3, padding=1)\n",
    "        self.bn4   = nn.BatchNorm2d(128)\n",
    "        self.pool  = nn.MaxPool2d(2,2)\n",
    "        self.act   = nn.LeakyReLU(0.1, inplace=True) \n",
    "        self.drop  = nn.Dropout(p_drop)\n",
    "        self.out   = nn.Linear(128, 128)\n",
    "        self._init()\n",
    "    def _init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.act(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.act(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.act(self.bn3(self.conv3(x))))\n",
    "        x = self.act(self.bn4(self.conv4(x)))\n",
    "        x = F.adaptive_avg_pool2d(x, (1,1))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.drop(x)\n",
    "        return self.out(x)  #(B,128)\n",
    "\n",
    "class VecField(nn.Module):\n",
    "    def __init__(self, dz=CFG.DZ, width=CFG.VF_WIDTH):\n",
    "        super().__init__()\n",
    "        self.nfe = 0\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dz+1, width), nn.SiLU(),\n",
    "            nn.Linear(width, width), nn.SiLU(),\n",
    "            nn.Linear(width, dz)\n",
    "        )\n",
    "        for m in self.mlp:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)\n",
    "    def forward(self, s, z):\n",
    "        self.nfe += 1\n",
    "        if z.ndim == 1: z = z.unsqueeze(0)\n",
    "        s_in = torch.full((z.shape[0],1), float(s), device=z.device, dtype=z.dtype)\n",
    "        x = torch.cat([z, s_in], dim=1)\n",
    "        return self.mlp(x)\n",
    "    def reset_nfe(self): self.nfe = 0\n",
    "#NODE\n",
    "class NODEBlock(nn.Module):\n",
    "    def __init__(self, dz=CFG.DZ, method=CFG.ODE_METHOD, rtol=CFG.ODE_RTOL, atol=CFG.ODE_ATOL, max_steps=CFG.ODE_MAX_STEPS):\n",
    "        super().__init__()\n",
    "        self.func = VecField(dz=dz, width=CFG.VF_WIDTH)\n",
    "        self.method = method\n",
    "        self.rtol   = rtol\n",
    "        self.atol   = atol\n",
    "        self.max_steps = max_steps\n",
    "        self.register_buffer('tspan', torch.tensor([0.0, 1.0], dtype=torch.float32))\n",
    "    def forward(self, z0):\n",
    "        z0_fp32 = z0.float()\n",
    "        self.func.reset_nfe()\n",
    "        zT = odeint(self.func, z0_fp32, self.tspan,\n",
    "                    method=self.method, rtol=self.rtol, atol=self.atol,\n",
    "                    options={'max_num_steps': int(self.max_steps)})\n",
    "        z1 = zT[-1]\n",
    "        return z1.to(z0.dtype), self.func.nfe\n",
    "\n",
    "class CNN_NODE(nn.Module):\n",
    "    def __init__(self, in_ch=1, dz=CFG.DZ, p_drop=CFG.DROPOUT_P):\n",
    "        super().__init__()\n",
    "        self.enc   = Encoder(in_ch=in_ch, p_drop=p_drop)\n",
    "        self.lin_z0= nn.Linear(128, dz)\n",
    "        self.node  = NODEBlock(dz=dz)\n",
    "        self.head  = nn.Linear(dz, 1)\n",
    "        nn.init.xavier_uniform_(self.lin_z0.weight); nn.init.zeros_(self.lin_z0.bias)\n",
    "        nn.init.xavier_uniform_(self.head.weight);  nn.init.zeros_(self.head.bias)\n",
    "    def forward(self, x):\n",
    "        h  = self.enc(x)\n",
    "        z0 = self.lin_z0(h)\n",
    "        z1, nfe = self.node(z0)\n",
    "        y  = self.head(z1).squeeze(1)\n",
    "        return y, nfe\n",
    "\n",
    "#LOSS and Metrics\n",
    "@torch.no_grad()\n",
    "def batch_corr(yhat, y):\n",
    "    yhat = yhat.float(); y = y.float()\n",
    "    yhat = yhat - yhat.mean(); y = y - y.mean()\n",
    "    num = (yhat*y).sum()\n",
    "    den = torch.sqrt((yhat*yhat).sum() * (y*y).sum() + 1e-12)\n",
    "    return (num/den).item() if den > 0 else 0.0\n",
    "\n",
    "def corr_loss(yhat, y):\n",
    "    yhat = yhat.float(); y = y.float()\n",
    "    yhat = yhat - yhat.mean()\n",
    "    y    = y - y.mean()\n",
    "    num = (yhat*y).sum()\n",
    "    den = torch.sqrt((yhat*yhat).sum() * (y*y).sum() + 1e-12)\n",
    "    c = num/(den+1e-12)\n",
    "    return 1.0 - c\n",
    "\n",
    "#Train-predict\n",
    "def train_epochs(model, loader, nepochs, opt=None, device=CFG.DEVICE):\n",
    "    model.to(device)\n",
    "    if CFG.CHANNELS_LAST:\n",
    "        model = model.to(memory_format=torch.channels_last)\n",
    "    model.train()\n",
    "    if opt is None:\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=CFG.LR, weight_decay=CFG.WEIGHT_DECAY)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(CFG.USE_SCALER and device=='cuda'))\n",
    "\n",
    "    for ep in range(1, nepochs+1):\n",
    "        t0 = time.time()\n",
    "        loss_meter, corr_meter, nfe_meter, nb = 0.0, 0.0, 0.0, 0\n",
    "        steps = 10\n",
    "        for xb, yb in loader:\n",
    "            steps += 2\n",
    "            if xb.ndim == 3: xb = xb.unsqueeze(0)\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True).float()\n",
    "            if CFG.CHANNELS_LAST and xb.ndim == 4:\n",
    "                xb = xb.contiguous(memory_format=torch.channels_last)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.amp.autocast(device_type='cuda', dtype=CFG.AMP_DTYPE, enabled=(device=='cuda')):\n",
    "                yhat, nfe = model(xb)\n",
    "                mse  = F.mse_loss(yhat.float(), yb)\n",
    "                cl   = corr_loss(yhat.float(), yb)\n",
    "                loss = CFG.LOSS_ALPHA_MSE*mse + (1.0-CFG.LOSS_ALPHA_MSE)*cl\n",
    "\n",
    "            if scaler.is_enabled():\n",
    "                scaler.scale(loss).backward()\n",
    "                if CFG.GRAD_CLIP_NORM is not None:\n",
    "                    scaler.unscale_(opt)\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), CFG.GRAD_CLIP_NORM)\n",
    "                scaler.step(opt); scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                if CFG.GRAD_CLIP_NORM is not None:\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), CFG.GRAD_CLIP_NORM)\n",
    "                opt.step()\n",
    "\n",
    "            loss_meter += loss.item() * yb.size(0)\n",
    "            corr_meter += batch_corr(yhat.detach(), yb) * yb.size(0)\n",
    "            nfe_meter  += float(nfe) * yb.size(0)\n",
    "            nb         += yb.size(0)\n",
    "\n",
    "        t1 = time.time()\n",
    "        print(f\"  epoch {ep}/{nepochs}  loss={loss_meter/nb:.6f}  corr≈{corr_meter/nb:.3f}  \"\n",
    "              f\"time={t1-t0:.1f}s  steps={steps}  NFE≈{nfe_meter/nb:.1f}\")\n",
    "    return opt, model\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, X, device=CFG.DEVICE, bs=8192):\n",
    "    model.eval(); model.to(device)\n",
    "    out = np.empty(X.shape[0], dtype=np.float32)\n",
    "    for i in range(0, X.shape[0], bs):\n",
    "        xb = torch.from_numpy(X[i:i+bs]).to(device, non_blocking=True)\n",
    "        if CFG.CHANNELS_LAST and xb.ndim == 4:\n",
    "            xb = xb.contiguous(memory_format=torch.channels_last)\n",
    "        yhat, _ = model(xb)\n",
    "        out[i:i+bs] = yhat.float().detach().cpu().numpy()\n",
    "    return out\n",
    "\n",
    "\n",
    "def newey_west_mean_t(y, lags=6):\n",
    "    y = pd.Series(y).astype(float).dropna()\n",
    "    if y.empty: return np.nan, np.nan\n",
    "    X = np.ones((len(y),1))\n",
    "    res = sm.OLS(y.values, X).fit()\n",
    "    V = cov_hac(res, nlags=lags)\n",
    "    mu = float(res.params[0])\n",
    "    se = float(np.sqrt(V[0,0])) if V is not None and V[0,0]>0 else np.nan\n",
    "    return mu, (mu/se if (se and se>0) else np.nan)\n",
    "\n",
    "def sharpe_ann(y):\n",
    "    y = pd.Series(y).astype(float).dropna()\n",
    "    s = y.std(ddof=1)\n",
    "    return (y.mean()/s)*np.sqrt(12.0) if s>0 else np.nan\n",
    "\n",
    "def _assign_ports_per_month_rank(s, n):\n",
    "    x = pd.Series(s); mask = x.notna(); xn = x[mask]\n",
    "    k = min(n, int(xn.nunique()))\n",
    "    if (k<2) or (len(xn)<2):\n",
    "        return pd.Series(pd.array([pd.NA]*len(x), dtype=\"Int64\"), index=x.index)\n",
    "    r = xn.rank(method='first')\n",
    "    nobs = len(xn)\n",
    "    port = ((r-1)*k/nobs).astype(int)+1\n",
    "    port = port.clip(1,k).astype(\"Int64\")\n",
    "    out = pd.Series(pd.array([pd.NA]*len(x), dtype=\"Int64\"), index=x.index)\n",
    "    out.loc[xn.index] = port\n",
    "    return out\n",
    "\n",
    "def _monthly_port_avgs(d, weight_col, pred_col, ret_col):\n",
    "    if weight_col is None:\n",
    "        g = (d.groupby(['month','port'], observed=True)\n",
    "              .agg(pred=(pred_col,'mean'), real=(ret_col,'mean'))\n",
    "              .reset_index())\n",
    "    else:\n",
    "        grp = d[['month','port',pred_col,ret_col,weight_col]].dropna()\n",
    "        grp = grp[grp[weight_col]>0]\n",
    "        for col in (pred_col, ret_col):\n",
    "            grp[f'w_{col}'] = grp[col]*grp[weight_col]\n",
    "        g = (grp.groupby(['month','port'], observed=True)\n",
    "                .agg(pred=('w_'+pred_col,'sum'),\n",
    "                    real=('w_'+ret_col,'sum'),\n",
    "                    w=(weight_col,'sum'))\n",
    "                .reset_index())\n",
    "        g['pred'] = np.where(g['w']>0, g['pred']/g['w'], np.nan)\n",
    "        g['real'] = np.where(g['w']>0, g['real']/g['w'], np.nan)\n",
    "        g = g.drop(columns='w')\n",
    "    return g\n",
    "\n",
    "def decile_table(meta, pred_col, ret_col, n_ports=10, weighting='oiw', lags=CFG.LAGS_NW, display_pct=CFG.DISPLAY_IN_PCT):\n",
    "    d = meta.copy()\n",
    "    if 'doi' not in d.columns: d['doi'] = 1.0\n",
    "    d = d.dropna(subset=['month', pred_col, ret_col])\n",
    "    d['month'] = pd.to_datetime(d['month'])\n",
    "    d['port'] = d.groupby('month', observed=True)[pred_col].transform(lambda s: _assign_ports_per_month_rank(s, n_ports))\n",
    "    d = d.dropna(subset=['port']); d['port'] = d['port'].astype('Int64')\n",
    "    weight_col = None if weighting.lower()=='ew' else 'doi'\n",
    "    g = _monthly_port_avgs(d, weight_col, pred_col, ret_col)\n",
    "    maxp = d.groupby('month', observed=True)['port'].max().rename('maxp')\n",
    "    gj   = g.merge(maxp, on='month', how='left')\n",
    "    def _ts(k, col):\n",
    "        return g[g['port']==k].set_index('month')[col].sort_index()\n",
    "    low_p  = _ts(1,'pred');  low_r  = _ts(1,'real')\n",
    "    high_p = gj[gj['port']==gj['maxp']].set_index('month')['pred'].sort_index()\n",
    "    high_r = gj[gj['port']==gj['maxp']].set_index('month')['real'].sort_index()\n",
    "\n",
    "    rows = []\n",
    "    labels = (['Low'] + list(range(2,n_ports)) + ['High'])\n",
    "    for lbl in labels:\n",
    "        if lbl=='Low': pr, rr = low_p, low_r\n",
    "        elif lbl=='High': pr, rr = high_p, high_r\n",
    "        else: pr, rr = _ts(int(lbl),'pred'), _ts(int(lbl),'real')\n",
    "        mu_pred = float(pd.Series(pr).dropna().mean()) if len(pr)>0 else np.nan\n",
    "        mu_real = float(pd.Series(rr).dropna().mean()) if len(rr)>0 else np.nan\n",
    "        tstat   = newey_west_mean_t(rr, lags=lags)[1]\n",
    "        sr      = sharpe_ann(rr)\n",
    "        rows.append({'Bucket': lbl, 'Pred.': mu_pred, 'Real.': mu_real, 't': tstat, 'SR': sr})\n",
    "\n",
    "    hml_pred = (high_p - low_p).dropna(); hml_real = (high_r - low_r).dropna()\n",
    "    rows.append({'Bucket':'HML',\n",
    "                'Pred.': float(hml_pred.mean()) if not hml_pred.empty else np.nan,\n",
    "                'Real.': float(hml_real.mean()) if not hml_real.empty else np.nan,\n",
    "                't': newey_west_mean_t(hml_real, lags=lags)[1],\n",
    "                'SR': sharpe_ann(hml_real)})\n",
    "    tbl = pd.DataFrame(rows).set_index('Bucket')\n",
    "    if display_pct:\n",
    "        for c in ['Pred.','Real.']: tbl[c] = 100.0*tbl[c]\n",
    "    return tbl\n",
    "\n",
    "#Option Characteristics\n",
    "def _monthly_port_wavg_for_chars(df, port_col, weight_col, char_cols):\n",
    "    keep = ['month', port_col] + char_cols + ([weight_col] if weight_col else [])\n",
    "    d = df[keep].dropna(subset=['month', port_col])\n",
    "    d['month'] = pd.to_datetime(d['month'])\n",
    "    if weight_col is None:\n",
    "        g = d.groupby(['month', port_col], observed=True)[char_cols].mean().reset_index()\n",
    "    else:\n",
    "        parts = []\n",
    "        for c in char_cols:\n",
    "            tmp = d[['month', port_col, c, weight_col]].dropna()\n",
    "            tmp = tmp[tmp[weight_col]>0]\n",
    "            tmp['wx'] = tmp[c]*tmp[weight_col]\n",
    "            g = (tmp.groupby(['month', port_col], observed=True)\n",
    "                    .agg(wx=('wx','sum'), w=(weight_col,'sum')).reset_index())\n",
    "            g[c] = np.where(g['w']>0, g['wx']/g['w'], np.nan)\n",
    "            parts.append(g[['month', port_col, c]])\n",
    "        g = parts[0]\n",
    "        for k in range(1, len(parts)): g = g.merge(parts[k], on=['month',port_col], how='outer')\n",
    "    return g\n",
    "\n",
    "def hofler_option_chars_table(meta, pred_col, char_cols=None, n_ports=10,\n",
    "                              weighting='oiw', lags=CFG.LAGS_NW,\n",
    "                              pct_cols=None, rnd=CFG.HOF_ROUND,\n",
    "                              scalers=None):\n",
    "    d = meta.copy()\n",
    "    if 'doi' not in d.columns: d['doi'] = 1.0\n",
    "    if char_cols is None: char_cols = CFG.OPTION_CHAR_COLS\n",
    "    char_cols = [c for c in char_cols if c in d.columns]\n",
    "    if len(char_cols)==0:\n",
    "        raise ValueError(\"no columns option characteristics at meta.\")\n",
    "    pct_cols = [] if pct_cols is None else [c for c in pct_cols if c in char_cols]\n",
    "    scalers  = {} if scalers  is None else {k:v for k,v in scalers.items() if k in char_cols}\n",
    "\n",
    "    d = d.dropna(subset=['month', pred_col])\n",
    "    d['month'] = pd.to_datetime(d['month'])\n",
    "    d['port'] = d.groupby('month', observed=True)[pred_col].transform(lambda s: _assign_ports_per_month_rank(s, n_ports))\n",
    "    d = d.dropna(subset=['port']); d['port'] = d['port'].astype('Int64')\n",
    "\n",
    "    weight_col = None if weighting.lower()=='ew' else 'doi'\n",
    "    gp = _monthly_port_wavg_for_chars(d, 'port', weight_col, char_cols)\n",
    "    maxp = d.groupby('month', observed=True)['port'].max().rename('maxp')\n",
    "    gp = gp.merge(maxp, on='month', how='left')\n",
    "\n",
    "    labels = (['Low'] + list(range(2, n_ports)) + ['High'])\n",
    "    out = {}\n",
    "    for lab in labels:\n",
    "        sel = (gp['port']==1) if lab=='Low' else ((gp['port']==gp['maxp']) if lab=='High' else (gp['port']==int(lab)))\n",
    "        m = gp[sel].drop(columns=['port','maxp']).set_index('month').sort_index()\n",
    "        out[lab] = m[char_cols].mean(axis=0, skipna=True).to_dict()\n",
    "\n",
    "    low_ts  = gp[gp['port']==1].set_index('month').sort_index()[char_cols]\n",
    "    high_ts = gp[gp['port']==gp['maxp']].set_index('month').sort_index()[char_cols]\n",
    "    idx = low_ts.index.intersection(high_ts.index)\n",
    "    hml_ts = (high_ts.loc[idx] - low_ts.loc[idx])\n",
    "\n",
    "    hml_mean, tstats = {}, {}\n",
    "    for c in char_cols:\n",
    "        mu, tstat = newey_west_mean_t(hml_ts[c], lags=lags)\n",
    "        hml_mean[c] = float(mu);  tstats[c] = float(tstat)\n",
    "\n",
    "    rows = [{'Bucket': lab, **out[lab]} for lab in labels]\n",
    "    rows.append({'Bucket':'HML', **hml_mean})\n",
    "    rows.append({'Bucket':'t',   **tstats})\n",
    "    tbl = pd.DataFrame(rows).set_index('Bucket')\n",
    "\n",
    "    #display scaling\n",
    "    base_rows = [r for r in tbl.index if r!='t']\n",
    "    for col, sc in scalers.items():\n",
    "        tbl.loc[base_rows, col] = tbl.loc[base_rows, col] * float(sc)\n",
    "\n",
    "    for c in pct_cols:\n",
    "        tbl.loc[base_rows, c] = 100.0 * tbl.loc[base_rows, c]\n",
    "    def _format_tbl(t):\n",
    "        t2 = t.copy()\n",
    "        t2.loc[base_rows, :] = t2.loc[base_rows, :].round(rnd)\n",
    "        t2 = t2.astype(object)  \n",
    "        t2.loc['t', :] = t.loc['t', :].apply(lambda x: f\"({x:.2f})\" if pd.notna(x) else \"(.)\").astype(object)\n",
    "        return t2\n",
    "    return tbl, _format_tbl(tbl)\n",
    "\n",
    "\n",
    "#shap analysis\n",
    "class EnsembleWrapper(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "    def forward(self, x):\n",
    "        preds = []\n",
    "        for m in self.models:\n",
    "            y, _ = m(x)\n",
    "            preds.append(y.unsqueeze(1))\n",
    "        yavg = torch.mean(torch.stack(preds, dim=0), dim=0)\n",
    "        return yavg\n",
    "\n",
    "#plots\n",
    "def _decile_split_and_plot(sv_arr, X_ex_cl, f_nhwc, save_dir, show=True, save=True):\n",
    "    import numpy as np, os, matplotlib.pyplot as plt\n",
    "    from IPython.display import display as ipy_display, Image as IPImage\n",
    "    preds = f_nhwc(X_ex_cl).ravel()\n",
    "    qL, qH = np.percentile(preds, [10,90])\n",
    "    L = np.where(preds<=qL)[0]; Hidx = np.where(preds>=qH)[0]\n",
    "    Lmap = sv_arr[L].mean(axis=0).sum(axis=-1)\n",
    "    Hmap = sv_arr[Hidx].mean(axis=0).sum(axis=-1)\n",
    "    Dmap = Hmap - Lmap\n",
    "    vmax = np.percentile(np.abs(Dmap), 99)+1e-12\n",
    "    fig, axs = plt.subplots(1,3,figsize=(14,4))\n",
    "    for ax, M, ttl in zip(axs, [Lmap,Hmap,Dmap], [\"Signed SHAP – Low decile\",\n",
    "                                                  \"Signed SHAP – High decile\",\n",
    "                                                  \"High − Low\"]):\n",
    "        im = ax.imshow(M, aspect='auto', cmap='RdBu_r', vmin=-vmax, vmax=vmax, interpolation='nearest')\n",
    "        ax.set_title(ttl); fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "        ax.set_xlabel(\"maturity bins\"); ax.set_ylabel(\"moneyness bins\")\n",
    "    out = os.path.join(save_dir, \"shap_decile_split.png\")\n",
    "    if save: fig.savefig(out, dpi=180, bbox_inches='tight')\n",
    "    if show: ipy_display(IPImage(filename=out)) if save else plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def _faithfulness_occlusion_report(sv_arr, X_ex_cl, f_nhwc, save_dir, show=True, save=True):\n",
    "    import numpy as np, os, matplotlib.pyplot as plt\n",
    "    from IPython.display import display as ipy_display, Image as IPImage\n",
    "    mass = np.abs(sv_arr).sum(axis=(0,3))   # (H,W)\n",
    "    thr = np.quantile(mass, 0.95)           # top-5%\n",
    "    roi = mass >= thr\n",
    "    X_mask = X_ex_cl.copy()\n",
    "    X_mask[..., roi] = 0.0                 \n",
    "    p0 = f_nhwc(X_ex_cl).ravel()\n",
    "    p1 = f_nhwc(X_mask).ravel()\n",
    "    drop = float(np.mean(np.abs(p1 - p0)))\n",
    "    print(f\"Faithfulness: mean |Δprediction| after masking top-5% cells = {drop:.3e}\")\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize=(5,4))\n",
    "    im = ax.imshow(roi.astype(float), aspect='auto', cmap='Greys')\n",
    "    ax.set_title(\"Masked ROI (top-5% mass cells)\")\n",
    "    out = os.path.join(save_dir, \"shap_masked_roi.png\")\n",
    "    if save: fig.savefig(out, dpi=180, bbox_inches='tight')\n",
    "    if show: ipy_display(IPImage(filename=out)) if save else plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def _roi_newey_west_share(sv_arr, ex_idx, meta, rows=None, cols=None, lags=6):\n",
    "    import numpy as np, pandas as pd\n",
    "    from math import isfinite\n",
    "    H, W = sv_arr.shape[1], sv_arr.shape[2]\n",
    "    R = np.zeros((H,W), dtype=bool)\n",
    "    if rows is None: rows = range(H)\n",
    "    if cols is None: cols = range(W)\n",
    "    R[np.ix_(list(rows), list(cols))] = True\n",
    "    absv = np.abs(sv_arr)                   #(N,H,W,C)\n",
    "    num = absv[:, R, :].sum(axis=(1,2))     \n",
    "    num = num.sum(axis=-1) if num.ndim==2 else num\n",
    "    den = absv.sum(axis=(1,2,3)) + 1e-12    \n",
    "    share = (num/den)                       \n",
    "    m = pd.DataFrame({\"month\": pd.to_datetime(meta.iloc[ex_idx]['month']).values, \"s\": share})\n",
    "    r = m.groupby(\"month\")[\"s\"].mean().dropna()\n",
    "    # \n",
    "    mu, t = newey_west_mean_t(r.values, lags=lags)\n",
    "    print(f\"ROI share mean={mu:.3%}  NW t={t:.2f}   (rows={list(rows)}, cols={list(cols)})\")\n",
    "    return mu, t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clone_model_no_inplace(src_model, in_ch, dz, p_drop): \n",
    "    dst = CNN_NODE(in_ch=in_ch, dz=dz, p_drop=p_drop)\n",
    "    dst.load_state_dict(copy.deepcopy(src_model.state_dict()))\n",
    "    for mod in dst.modules():\n",
    "        if isinstance(mod, (nn.ReLU, nn.LeakyReLU)):\n",
    "            mod.inplace = False\n",
    "    return dst\n",
    "\n",
    "\n",
    "def run_shap_analysis(Xz, meta, models_dict, first_oos, device=CFG.DEVICE):\n",
    "\n",
    "    from IPython.display import display as ipy_display, Image as IPImage\n",
    "\n",
    "    outdir = getattr(CFG, \"SHAP_SAVE_DIR\", \"./shap_out\")\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    SHOW         = getattr(CFG, \"SHAP_SHOW_FIGS\", True)\n",
    "    SAVE         = getattr(CFG, \"SHAP_SAVE_FIGS\", True)\n",
    "    SAVE_STATS   = getattr(CFG, \"SHAP_SAVE_STATS\", False)\n",
    "    TOPK         = int(getattr(CFG, \"SHAP_STATS_TOPK\", 10))\n",
    "    MAX_EVALS    = getattr(CFG, \"SHAP_MAX_EVALS\", None)    \n",
    "    # extra toggles\n",
    "    DO_DECILES   = getattr(CFG, \"SHAP_DO_DECILES\", True)\n",
    "    DO_FAITHFUL  = getattr(CFG, \"SHAP_DO_FAITHFULNESS\", True)\n",
    "    DO_ROI_NW    = getattr(CFG, \"SHAP_DO_ROI_NW\", True)\n",
    "\n",
    "    def _nhwc_predictor(x_nhwc: np.ndarray) -> np.ndarray:\n",
    "        x = np.array(x_nhwc, dtype=np.float32)\n",
    "        if x.ndim == 3: x = x[None, ...]\n",
    "        x_nchw = np.transpose(x, (0,3,1,2))\n",
    "        xb = torch.from_numpy(x_nchw)\n",
    "        if getattr(CFG, \"CHANNELS_LAST\", True) and xb.ndim == 4:\n",
    "            xb = xb.contiguous(memory_format=torch.channels_last)\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            preds = []\n",
    "            for s in models_dict.keys():\n",
    "                m = models_dict[s].eval().to(device)\n",
    "                y, _ = m(xb)\n",
    "                preds.append(y.unsqueeze(1))\n",
    "            yavg = torch.mean(torch.stack(preds, dim=0), dim=0) \n",
    "        return yavg.detach().cpu().numpy()\n",
    "\n",
    "    def _normalize_sv(values):\n",
    "        sv_raw = values if not isinstance(values, list) else values[0]\n",
    "        arr = np.asarray(sv_raw)\n",
    "        if arr.ndim == 5:  #(N,H,W,C,O) -> (N,H,W,C*O)\n",
    "            N,H,W,C,O = arr.shape\n",
    "            arr = arr.reshape(N,H,W,C*O)\n",
    "        if arr.ndim == 3:  #(N,H,W) -> (N,H,W,1)\n",
    "            arr = arr[..., np.newaxis]\n",
    "        while arr.ndim > 4 and arr.shape[-1] == 1:\n",
    "            arr = arr[..., 0]\n",
    "        if arr.ndim != 4:\n",
    "            raise ValueError(f\"SHAP: shape shape {arr.shape} (it espect 4D).\")\n",
    "        return arr\n",
    "\n",
    "    def _save_or_show(fig, path_png):\n",
    "        if SAVE: fig.savefig(path_png, dpi=180, bbox_inches='tight')\n",
    "        if SHOW:\n",
    "            if SAVE: ipy_display(IPImage(filename=path_png))\n",
    "            else:    plt.show()\n",
    "        plt.close(fig)\n",
    "\n",
    "    def _decile_split_and_plot(sv_arr, X_ex_cl):\n",
    "        preds = _nhwc_predictor(X_ex_cl).ravel()\n",
    "        qL, qH = np.percentile(preds, [10,90])\n",
    "        L = np.where(preds<=qL)[0]; Hidx = np.where(preds>=qH)[0]\n",
    "        Lmap = sv_arr[L].mean(axis=0).sum(axis=-1) if len(L)>0 else np.zeros(sv_arr.shape[1:3])\n",
    "        Hmap = sv_arr[Hidx].mean(axis=0).sum(axis=-1) if len(Hidx)>0 else np.zeros(sv_arr.shape[1:3])\n",
    "        Dmap = Hmap - Lmap\n",
    "        vmax = np.percentile(np.abs(Dmap), 99)+1e-12\n",
    "        fig, axs = plt.subplots(1,3,figsize=(14,4))\n",
    "        for ax, M, ttl in zip(axs, [Lmap,Hmap,Dmap], [\"Signed SHAP – Low decile\",\n",
    "                                                      \"Signed SHAP – High decile\",\n",
    "                                                      \"High − Low\"]):\n",
    "            im = ax.imshow(M, aspect='auto', cmap='RdBu_r', vmin=-vmax, vmax=vmax, interpolation='nearest')\n",
    "            ax.set_title(ttl); fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "            ax.set_xlabel(\"maturity bins\"); ax.set_ylabel(\"moneyness bins\")\n",
    "        _save_or_show(fig, os.path.join(outdir, \"shap_decile_split.png\"))\n",
    "\n",
    "    def _faithfulness_occlusion_report(sv_arr, X_ex_cl):\n",
    "      \n",
    "        mass = np.abs(sv_arr).sum(axis=(0,3))     #(H,W)\n",
    "        thr  = np.quantile(mass, 0.95)\n",
    "        roi  = mass >= thr                         #(H,W) True = high-importance\n",
    "        mask4 = (~roi)[None, :, :, None].astype(X_ex_cl.dtype)  # (1,H,W,1)\n",
    "        X_mask = X_ex_cl * mask4                   \n",
    "\n",
    "        p0 = _nhwc_predictor(X_ex_cl).ravel()\n",
    "        p1 = _nhwc_predictor(X_mask).ravel()\n",
    "        drop = float(np.mean(np.abs(p1 - p0)))\n",
    "        print(f\"Faithfulness: mean |Δprediction| after masking top-5% cells = {drop:.3e}\")\n",
    "        fig, ax = plt.subplots(1,1,figsize=(5,4))\n",
    "        im = ax.imshow(roi.astype(float), aspect='auto', cmap='Greys')\n",
    "        ax.set_title(\"Masked ROI (top-5% mass cells)\")\n",
    "        _save_or_show(fig, os.path.join(outdir, \"shap_masked_roi.png\"))\n",
    "\n",
    "    def _roi_newey_west_share(sv_arr, ex_idx, rows=None, cols=None, lags=CFG.LAGS_NW):\n",
    "        H, W = sv_arr.shape[1], sv_arr.shape[2]\n",
    "        R = np.zeros((H,W), dtype=bool)\n",
    "        if rows is None: rows = range(H)\n",
    "        if cols is None: cols = range(W)\n",
    "        R[np.ix_(list(rows), list(cols))] = True\n",
    "        absv = np.abs(sv_arr)\n",
    "        num = absv[:, R, :].sum(axis=(1,2))\n",
    "        num = num.sum(axis=-1) if num.ndim==2 else num\n",
    "        den = absv.sum(axis=(1,2,3)) + 1e-12\n",
    "        share = (num/den)\n",
    "        m = pd.DataFrame({\"month\": pd.to_datetime(meta.iloc[ex_idx]['month']).values, \"s\": share})\n",
    "        r = m.groupby(\"month\")[\"s\"].mean().dropna()\n",
    "        mu, t = newey_west_mean_t(r.values, lags=lags)\n",
    "        print(f\"ROI share mean={mu:.3%}  NW t={t:.2f}   (rows={list(rows)}, cols={list(cols)})\")\n",
    "        return mu, t\n",
    "\n",
    "    def _hml_series_from_preds(meta_df, pred_vec, idx_vec, name=\"pred_tmp\",\n",
    "                              n_ports=CFG.PORTS_N, weighting='oiw'):\n",
    "        d = meta_df.copy()\n",
    "        d[name] = np.nan\n",
    "\n",
    "        d.loc[idx_vec, name] = np.asarray(pred_vec, dtype=np.float32)\n",
    "        d = d.dropna(subset=['month', name, CFG.TARGET_COL])\n",
    "        d['month'] = pd.to_datetime(d['month'])\n",
    "        d['port'] = d.groupby('month', observed=True)[name].transform(\n",
    "            lambda s: _assign_ports_per_month_rank(s, n_ports))\n",
    "        d = d.dropna(subset=['port']); d['port'] = d['port'].astype('Int64')\n",
    "        weight_col = None if weighting.lower()=='ew' else 'doi'\n",
    "        g = _monthly_port_avgs(d, weight_col, name, CFG.TARGET_COL)\n",
    "        maxp = d.groupby('month', observed=True)['port'].max().rename('maxp')\n",
    "        gj   = g.merge(maxp, on='month', how='left').set_index('month').sort_index()\n",
    "        low_r  = gj[gj['port']==1]['real']\n",
    "        high_r = gj[gj['port']==gj['maxp']]['real']\n",
    "        return (high_r - low_r).dropna().astype(float)  # monthly H−L\n",
    "    ins_idx = np.where(pd.to_datetime(meta['month']).lt(first_oos).values)[0]\n",
    "    if ins_idx.size == 0:\n",
    "        print(\"SHAP: no in-sample for background.\")\n",
    "        return\n",
    "    bg_idx = np.random.choice(ins_idx, size=min(getattr(CFG, \"SHAP_BG_SAMPLES\", 64), ins_idx.size), replace=False)\n",
    "\n",
    "    oos_mask = np.isfinite(meta['rb_cnn_node'].to_numpy(np.float32))\n",
    "    ex_idx_all = np.where(oos_mask)[0]\n",
    "    if ex_idx_all.size == 0:\n",
    "        print(\"SHAP: no OOS forecasts for explanation.\")\n",
    "        return\n",
    "    ex_idx = np.random.choice(ex_idx_all, size=min(getattr(CFG, \"SHAP_EXPLAIN_SAMPLES\", 64), ex_idx_all.size), replace=False)\n",
    "\n",
    "    X_bg_cl = np.transpose(Xz[bg_idx].astype(np.float32), (0,2,3,1))  #(M,H,W,C)\n",
    "    X_ex_cl = np.transpose(Xz[ex_idx].astype(np.float32), (0,2,3,1))\n",
    "    H, W, C = X_ex_cl.shape[1], X_ex_cl.shape[2], X_ex_cl.shape[3]\n",
    "\n",
    "    kr = max(1, H // 4); kc = max(1, W // 4)\n",
    "    masker = shap.maskers.Image(f\"blur({kr},{kc})\", X_ex_cl[0].shape)\n",
    "    explainer = shap.Explainer(_nhwc_predictor, masker, algorithm=\"partition\")\n",
    "    sv = explainer(X_ex_cl) if MAX_EVALS is None else explainer(X_ex_cl, max_evals=(4*H*W if MAX_EVALS==\"auto\" else int(MAX_EVALS)))\n",
    "    sv_arr = _normalize_sv(sv.values)  # (N,H,W,CH)\n",
    "\n",
    "    abs_vals   = np.abs(sv_arr)\n",
    "    total_mass = abs_vals.sum() + 1e-12\n",
    "    mass_map   = abs_vals.sum(axis=(0,3)) / total_mass   # (H,W)\n",
    "    row_share  = mass_map.sum(axis=1)\n",
    "    col_share  = mass_map.sum(axis=0)\n",
    "    flat = mass_map.ravel()\n",
    "    idxs = np.argsort(flat)[::-1][:TOPK]\n",
    "    topk = [(int(i // W), int(i % W), float(flat[i])) for i in idxs]\n",
    "    share_topk = float(flat[idxs].sum())\n",
    "    global_mean_abs = float(abs_vals.mean())\n",
    "\n",
    "    #3×3 coarse\n",
    "    bins = 3\n",
    "    r_edges = np.linspace(0, H, bins+1, dtype=int); c_edges = np.linspace(0, W, bins+1, dtype=int)\n",
    "    coarse = np.zeros((bins, bins), dtype=float)\n",
    "    for i in range(bins):\n",
    "        for j in range(bins):\n",
    "            coarse[i,j] = mass_map[r_edges[i]:r_edges[i+1], c_edges[j]:c_edges[j+1]].sum()\n",
    "    coarse_df = pd.DataFrame(np.round(coarse*100, 2),\n",
    "                            index=[f\"R{i+1}\" for i in range(bins)],\n",
    "                            columns=[f\"C{j+1}\" for j in range(bins)])\n",
    "\n",
    "    print(\"\\nSHAP — summary stats\")\n",
    "    print(f\"  global mean |SHAP|: {global_mean_abs:.4e}\")\n",
    "    print(f\"  top-{TOPK} cells share of attribution mass: {share_topk*100:.1f}%\")\n",
    "    print(f\"  most important row index: {int(np.argmax(row_share))}  (share={row_share.max()*100:.1f}%)\")\n",
    "    print(f\"  most important col index: {int(np.argmax(col_share))}  (share={col_share.max()*100:.1f}%)\")\n",
    "    print(\"\\n  3×3 coarse share (% of mass) by row×col bins:\")\n",
    "    print(coarse_df.to_string(index=True))\n",
    "\n",
    "    if SAVE_STATS:\n",
    "        pd.DataFrame({\"row\":np.arange(H), \"share\":row_share}).to_csv(os.path.join(outdir, \"shap_row_share.csv\"), index=False)\n",
    "        pd.DataFrame({\"col\":np.arange(W), \"share\":col_share}).to_csv(os.path.join(outdir, \"shap_col_share.csv\"), index=False)\n",
    "        pd.DataFrame(topk, columns=[\"row\",\"col\",\"share\"]).to_csv(os.path.join(outdir, \"shap_topk_cells.csv\"), index=False)\n",
    "        coarse_df.to_csv(os.path.join(outdir, \"shap_coarse_3x3.csv\"))\n",
    "\n",
    "    #Plots (SHAP)\n",
    "    X_mean2d = X_ex_cl.mean(axis=(0,3))                    # (H,W)\n",
    "    signed_map = sv_arr.mean(axis=0).sum(axis=-1)          # (H,W)\n",
    "    vmax = np.percentile(np.abs(signed_map), 99) + 1e-12\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    im0 = axs[0].imshow(X_mean2d, aspect='auto', cmap='gray', interpolation='nearest')\n",
    "    axs[0].set_title(\"Mean IV surface (OOS samples)\")\n",
    "    fig.colorbar(im0, ax=axs[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "    im1 = axs[1].imshow(signed_map, aspect='auto', cmap='RdBu_r',\n",
    "                        vmin=-vmax, vmax=vmax, interpolation='nearest')\n",
    "    axs[1].set_title(\"Signed SHAP (mean across OOS)\")\n",
    "    fig.colorbar(im1, ax=axs[1], fraction=0.046, pad=0.04)\n",
    "    for ax in axs:\n",
    "        ax.set_xlabel(\"maturity bins\"); ax.set_ylabel(\"moneyness bins\")\n",
    "        ax.set_xticks(np.linspace(0, W-1, min(W, 6)).astype(int))\n",
    "        ax.set_yticks(np.linspace(0, H-1, min(H, 6)).astype(int))\n",
    "    _save_or_show(fig, os.path.join(outdir, \"shap_signed_vs_surface.png\"))\n",
    "\n",
    "    fig2, ax2 = plt.subplots(1, 1, figsize=(6.5, 4))\n",
    "    im2 = ax2.imshow(100*mass_map, aspect='auto', cmap='magma', interpolation='nearest')\n",
    "    ax2.set_title(\"Attribution mass share (%)\")\n",
    "    fig2.colorbar(im2, ax=ax2, fraction=0.046, pad=0.04)\n",
    "    ax2.set_xlabel(\"maturity bins\"); ax2.set_ylabel(\"moneyness bins\")\n",
    "    ax2.set_xticks(np.linspace(0, W-1, min(W, 6)).astype(int))\n",
    "    ax2.set_yticks(np.linspace(0, H-1, min(H, 6)).astype(int))\n",
    "    _save_or_show(fig2, os.path.join(outdir, \"shap_mass_share.png\"))\n",
    "\n",
    "\n",
    "    try:\n",
    "        oos_idx_for_hist = ex_idx_all\n",
    "\n",
    "        ens_vec_all = meta['rb_cnn_node'].to_numpy(np.float32)\n",
    "        ens_vec_oos = ens_vec_all[oos_idx_for_hist]\n",
    "        hml_ens = _hml_series_from_preds(meta, ens_vec_oos, oos_idx_for_hist,\n",
    "                                        name='rb_cnn_node', weighting='oiw')\n",
    "        sr_ens = sharpe_ann(hml_ens)\n",
    "\n",
    "        indiv_srs = []\n",
    "        for s, m in models_dict.items():\n",
    "            yhat_oos = predict(m, Xz[oos_idx_for_hist], device=device,\n",
    "                              bs=max(8192, getattr(CFG, \"BATCH_SIZE\", 8192)))\n",
    "            hml_ind = _hml_series_from_preds(meta, yhat_oos, oos_idx_for_hist,\n",
    "                                            name=f'pred_{s}', weighting='oiw')\n",
    "            sr_ind = sharpe_ann(hml_ind)\n",
    "            if np.isfinite(sr_ind): indiv_srs.append(float(sr_ind))\n",
    "\n",
    "        fig3, ax3 = plt.subplots(1,1, figsize=(7.2, 4.0))\n",
    "        if len(indiv_srs) > 0:\n",
    "            nbins = min(12, max(5, len(indiv_srs))) \n",
    "            ax3.hist(indiv_srs, bins=nbins, color='#0b3c5d', alpha=0.95, label='Individual')\n",
    "        if np.isfinite(sr_ens):\n",
    "            ax3.axvline(sr_ens, color='limegreen', linewidth=10, label='Ensemble', alpha=0.9)\n",
    "        ax3.set_xlabel(\"Annualized Sharpe ratio\"); ax3.set_ylabel(\"Frequency\")\n",
    "        ax3.set_title(\"Sharpe ratios of long–short portfolios (OIW)\")\n",
    "        ax3.legend()\n",
    "        xs = indiv_srs + ([sr_ens] if np.isfinite(sr_ens) else [])\n",
    "        if len(xs) > 0:\n",
    "            xmin, xmax = min(xs), max(xs)\n",
    "            pad = 0.1*(xmax - xmin + 1e-9)\n",
    "            ax3.set_xlim(xmin - pad, xmax + pad)\n",
    "        _save_or_show(fig3, os.path.join(outdir, \"sharpe_histogram_oiw.png\"))\n",
    "\n",
    "        print(f\"Figure3-style: Ensemble Sharpe (OIW) = {sr_ens:.2f}; \"\n",
    "              f\"Individual mean={np.mean(indiv_srs) if len(indiv_srs)>0 else float('nan'):.2f} (N={len(indiv_srs)})\")\n",
    "    except Exception as e_fig3:\n",
    "        print(f\"Sharpe histogram (Figure 3-style) failed: {e_fig3}\")\n",
    "\n",
    "    if DO_DECILES:\n",
    "        _decile_split_and_plot(sv_arr, X_ex_cl)\n",
    "    if DO_FAITHFUL:\n",
    "        _faithfulness_occlusion_report(sv_arr, X_ex_cl)\n",
    "    if DO_ROI_NW:\n",
    "        r0, c0 = int(np.argmax(row_share)), int(np.argmax(col_share))\n",
    "        r_low, r_high = max(0, r0-1), min(H, r0+2)\n",
    "        c_low, c_high = max(0, c0-1), min(W, c0+2)\n",
    "        _roi_newey_west_share(sv_arr, ex_idx, rows=range(r_low, r_high), cols=range(c_low, c_high), lags=getattr(CFG, \"LAGS_NW\", 6))\n",
    "\n",
    "    return {\n",
    "        \"row_share\": row_share,\n",
    "        \"col_share\": col_share,\n",
    "        \"topk\": topk,\n",
    "        \"coarse_3x3\": coarse,\n",
    "        \"global_mean_abs\": global_mean_abs\n",
    "    }\n",
    "LIME\n",
    "def run_lime_analysis(Xz, meta, models_dict, first_oos, device=CFG.DEVICE):\n",
    "\n",
    "    import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "    SHOW = getattr(CFG, \"LIME_SHOW_FIGS\", True)\n",
    "    SAVE = getattr(CFG, \"LIME_SAVE_FIGS\", False)\n",
    "\n",
    "    def _save_or_show(fig, fname=None):\n",
    "        if SAVE and fname:\n",
    "            os.makedirs(outdir, exist_ok=True)\n",
    "            fig.savefig(os.path.join(outdir, fname), dpi=180, bbox_inches='tight')\n",
    "        if SHOW:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.show()\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "    try:\n",
    "        from lime.lime_image import LimeImageExplainer\n",
    "        from skimage.segmentation import mark_boundaries\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"missing lime / scikit-image: pip install lime scikit-image\") from e\n",
    "\n",
    "    outdir = getattr(CFG, \"LIME_SAVE_DIR\", \"./lime_out\")\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # ---------------- Helpers ----------------\n",
    "    def _predict_nhwc(x_nhwc: np.ndarray) -> np.ndarray:\n",
    "        x = np.array(x_nhwc, dtype=np.float32)\n",
    "        if x.ndim == 3: x = x[None, ...]\n",
    "        bs = int(getattr(CFG, \"LIME_BATCH_SIZE\", 2048))\n",
    "        N = x.shape[0]\n",
    "        out = np.empty((N,1), dtype=np.float32)\n",
    "        for i in range(0, N, bs):\n",
    "            xb = x[i:i+bs]\n",
    "            x_nchw = np.transpose(xb, (0,3,1,2))               # NHWC -> NCHW\n",
    "            xt = torch.from_numpy(x_nchw).to(device, non_blocking=True)\n",
    "            if getattr(CFG, \"CHANNELS_LAST\", True) and xt.ndim == 4:\n",
    "                xt = xt.contiguous(memory_format=torch.channels_last)\n",
    "            with torch.no_grad():\n",
    "                preds = []\n",
    "                for s in models_dict.keys():\n",
    "                    m = models_dict[s].eval().to(device)\n",
    "                    y, _ = m(xt)                                # (B,)\n",
    "                    preds.append(y.unsqueeze(1))\n",
    "                yavg = torch.mean(torch.stack(preds, dim=0), dim=0)  # (B,1)\n",
    "            out[i:i+bs] = yavg.detach().cpu().numpy().astype(np.float32)\n",
    "        return out\n",
    "\n",
    "    def _pack_to_rgb_from_sample(X1_chw: np.ndarray) -> np.ndarray:\n",
    "        C,H,W = X1_chw.shape\n",
    "        x_hwc = np.transpose(X1_chw, (1,2,0))      # -> H,W,C\n",
    "        rgb = np.zeros((H,W,3), dtype=np.float32)\n",
    "        rgb[...,0] = x_hwc[...,0]\n",
    "        if C >= 2:\n",
    "            rgb[...,1] = x_hwc[...,1]\n",
    "        return rgb\n",
    "\n",
    "    def _unpack_from_rgb_to_nhwc(imgs_rgb: np.ndarray, C_in: int) -> np.ndarray:\n",
    "\n",
    "        if C_in == 1:\n",
    "            return imgs_rgb[..., :1].astype(np.float32)\n",
    "        else:\n",
    "            return imgs_rgb[..., :2].astype(np.float32)\n",
    "\n",
    "    def _grid_segmentation(image_hw3: np.ndarray) -> np.ndarray:\n",
    "        H, W = image_hw3.shape[0], image_hw3.shape[1]\n",
    "        return np.arange(H*W, dtype=np.int32).reshape(H, W)\n",
    "\n",
    "    def _classifier_for_lime(imgs_rgb: np.ndarray) -> np.ndarray:\n",
    "        C_in = int(Xz.shape[1])\n",
    "        x_nhwc = _unpack_from_rgb_to_nhwc(imgs_rgb, C_in=C_in)\n",
    "        y = _predict_nhwc(x_nhwc).reshape(-1)  # (N,)\n",
    "\n",
    "        center = getattr(_classifier_for_lime, \"_center\", float(y.mean()))\n",
    "        scale  = float(np.std(y) + 1e-6)\n",
    "        z = (y - center) / scale\n",
    "        p1 = 1.0 / (1.0 + np.exp(-z))\n",
    "        p0 = 1.0 - p1\n",
    "        return np.stack([p0, p1], axis=1).astype(np.float32)\n",
    "\n",
    "    oos_mask = np.isfinite(meta['rb_cnn_node'].to_numpy(np.float32))\n",
    "    ex_idx_all = np.where(oos_mask)[0]\n",
    "    if ex_idx_all.size == 0:\n",
    "        print(\"LIME: not enough OOS forecasts.\")\n",
    "        return\n",
    "\n",
    "    scores = meta.loc[ex_idx_all, 'rb_cnn_node'].astype(float).to_numpy()\n",
    "    order  = np.argsort(scores)\n",
    "    low_i  = ex_idx_all[order[0]]\n",
    "    high_i = ex_idx_all[order[-1]]\n",
    "    mid_i  = ex_idx_all[order[len(order)//2]]\n",
    "    candidates = [('low', low_i), ('mid', mid_i), ('high', high_i)]\n",
    "\n",
    "    H, W = Xz.shape[2], Xz.shape[3]\n",
    "    mode = str(getattr(CFG, \"LIME_SEGMENTATION_MODE\", \"grid\")).lower()\n",
    "\n",
    "    for tag, idx in candidates:\n",
    "        X1 = Xz[idx]                                   # (C,H,W)\n",
    "        x_rgb = _pack_to_rgb_from_sample(X1)           # (H,W,3)\n",
    "\n",
    "        C_in = int(Xz.shape[1])\n",
    "        x_nhwc_full = np.transpose(X1, (1,2,0)).astype(np.float32)  # H,W,C\n",
    "        y0 = float(_predict_nhwc(x_nhwc_full[None, ...])[0,0])\n",
    "        _classifier_for_lime._center = y0\n",
    "\n",
    "        #Segmentation fn\n",
    "        if mode == 'grid':\n",
    "            segmentation_fn = _grid_segmentation\n",
    "        else:\n",
    "            from skimage.segmentation import slic\n",
    "            def segmentation_fn(im):\n",
    "                return slic(im, n_segments=int(getattr(CFG, \"LIME_SLIC_SEGMENTS\", 100)),\n",
    "                            compactness=0.1, sigma=1, start_label=0)\n",
    "\n",
    "        explainer = LimeImageExplainer(random_state=123)\n",
    "        explanation = explainer.explain_instance(\n",
    "            image=x_rgb,\n",
    "            classifier_fn=_classifier_for_lime,\n",
    "            top_labels=2,\n",
    "            hide_color=0.0, \n",
    "            num_samples=int(getattr(CFG, \"LIME_NUM_SAMPLES\", 1500)),\n",
    "            segmentation_fn=segmentation_fn\n",
    "        )\n",
    "\n",
    "        label = 1 \n",
    "        num_feats = int(getattr(CFG, \"LIME_NUM_FEATURES\", 12))\n",
    "\n",
    "\n",
    "        img_pos,  mask_pos  = explanation.get_image_and_mask(label, positive_only=True,\n",
    "                                                            num_features=num_feats, hide_rest=False)\n",
    "        img_both, mask_both = explanation.get_image_and_mask(label, positive_only=False,\n",
    "                                                            num_features=num_feats, hide_rest=False)\n",
    "\n",
    "        def _save_overlay(base_rgb, mask, fname, title):\n",
    "            fig, ax = plt.subplots(1,1, figsize=(6,5))\n",
    "            ax.imshow(mark_boundaries(base_rgb, mask))\n",
    "            dt = pd.to_datetime(meta.iloc[idx]['month']).date() if 'month' in meta.columns else 'n/a'\n",
    "            ax.set_title(title)\n",
    "            ax.set_xlabel(f\"index={idx} | month={dt} | y0={y0:.4f}\")\n",
    "            ax.axis('off')\n",
    "            _save_or_show(fig, fname if SAVE else None)\n",
    "\n",
    "\n",
    "        _save_overlay(x_rgb, mask_pos,  f\"lime_{tag}_pos_{idx}.png\",\n",
    "                      f\"LIME (positive-only) — {tag.upper()} decile\")\n",
    "        _save_overlay(x_rgb, mask_both, f\"lime_{tag}_both_{idx}.png\",\n",
    "                      f\"LIME (pos/neg) — {tag.upper()} decile\")\n",
    "\n",
    "        if mode == 'grid':\n",
    "            sp_weights = dict(explanation.local_exp[label]) \n",
    "            Wgrid = np.zeros((H,W), dtype=np.float32)\n",
    "            for r in range(H):\n",
    "                for c in range(W):\n",
    "                    sp = r*W + c\n",
    "                    Wgrid[r,c] = sp_weights.get(sp, 0.0)\n",
    "\n",
    "            vmax = np.percentile(np.abs(Wgrid), 99) + 1e-12\n",
    "            fig, ax = plt.subplots(1,1, figsize=(6,5))\n",
    "            im = ax.imshow(Wgrid, cmap='RdBu_r', vmin=-vmax, vmax=vmax, aspect='auto')\n",
    "            ax.set_title(f\"LIME local weights per cell — {tag.upper()} (index={idx})\")\n",
    "            ax.set_xlabel(\"maturity bins\"); ax.set_ylabel(\"moneyness bins\")\n",
    "            fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "            _save_or_show(fig, f\"lime_{tag}_weights_grid_{idx}.png\")\n",
    "\n",
    "            plt.close(fig)\n",
    "\n",
    "    print(f\"LIME: finished files saved at: {os.path.abspath(outdir)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    dev = CFG.DEVICE\n",
    "    print(f\"CUDA: {torch.cuda.get_device_name(0) if dev=='cuda' else 'CPU'} | AMP={CFG.AMP_DTYPE} | channels_last={CFG.CHANNELS_LAST} | compile={CFG.TORCH_COMPILE}\")\n",
    "    Xz = load_Xz(CFG.XZ_PATH)            #(N,1,H,W)\n",
    "    meta = load_meta(CFG.META_PATH).copy()\n",
    "    if 'doi' not in meta.columns: meta['doi'] = 1.0\n",
    "    assert len(meta)==Xz.shape[0], \"len(meta) != N of Xz\"\n",
    "    meta['month'] = pd.to_datetime(meta['month'])\n",
    "    y = meta[CFG.TARGET_COL].to_numpy(np.float32)\n",
    "\n",
    "    print(f\"Xz finite ratio={float(np.isfinite(Xz).mean()):.4f} | y finite ratio={float(np.isfinite(y).mean()):.4f}\")\n",
    "    Xz = identity_stack(Xz, add_mask=CFG.ADD_MASK_CHANNEL)  # NaN->0 (no scaling)\n",
    "    in_ch = Xz.shape[1]\n",
    "    months = np.sort(meta['month'].unique())\n",
    "    first_oos = pd.to_datetime(meta['month'].min()) + pd.DateOffset(years=7)\n",
    "    years_oos = sorted(pd.Series(months[months>=first_oos]).dt.year.unique())\n",
    "    if CFG.MODE.upper()=='FAST' and len(years_oos)>2:\n",
    "        years_oos = years_oos[-2:]\n",
    "    print(f\"Mode={CFG.MODE} | OOS years: {len(years_oos)} -> {years_oos[0]}..{years_oos[-1]}\")\n",
    "\n",
    "    rb_hat = np.full(len(meta), np.nan, dtype=np.float32)\n",
    "\n",
    "    models, opts = {}, {}\n",
    "    for s in CFG.SEEDS:\n",
    "        torch.manual_seed(s); np.random.seed(s)\n",
    "        m = CNN_NODE(in_ch=in_ch, dz=CFG.DZ, p_drop=CFG.DROPOUT_P)\n",
    "        if CFG.TORCH_COMPILE and hasattr(torch, 'compile'):\n",
    "            m = torch.compile(m, fullgraph=False, dynamic=True)\n",
    "        o = torch.optim.AdamW(m.parameters(), lr=CFG.LR, weight_decay=CFG.WEIGHT_DECAY)\n",
    "        models[s], opts[s] = m, o\n",
    "\n",
    "    for i, Y in enumerate(tqdm(years_oos, desc=\"OOS years\")):\n",
    "        start_Y = pd.Timestamp(year=Y, month=1, day=1)\n",
    "        start_next = pd.Timestamp(year=Y+1, month=1, day=1)\n",
    "        tr_idx = np.where(meta['month'].lt(start_Y).values)[0]\n",
    "        te_idx = np.where((meta['month']>=start_Y) & (meta['month']<start_next) & (meta['month']>=first_oos))[0]\n",
    "        if tr_idx.size==0 or te_idx.size==0: continue\n",
    "\n",
    "        X_tr = Xz[tr_idx]\n",
    "        y_tr = y[tr_idx]\n",
    "        good = np.isfinite(y_tr)\n",
    "        if (~good).sum()>0:\n",
    "            print(f\"Warning: {(~good).sum()} μη-περατά targets — αγνοούνται στο training.\")\n",
    "        X_tr, y_tr = X_tr[good], y_tr[good]\n",
    "        loader = make_loader(X_tr, y_tr, bs=CFG.BATCH_SIZE, shuffle=True, device=dev)\n",
    "\n",
    "        ne = CFG.EPOCHS_INIT if i==0 else CFG.EPOCHS_PER_YEAR\n",
    "        print(f\"\\n=== Year {Y} | train_n={y_tr.size:,} | test_n={te_idx.size:,} | epochs={ne} ===\")\n",
    "        t0 = time.time()\n",
    "        for s in models.keys():\n",
    "            opts[s], models[s] = train_epochs(models[s], loader, nepochs=ne, opt=opts[s], device=dev)\n",
    "        t1 = time.time()\n",
    "        print(f\"Train time (year {Y}): {t1-t0:.1f}s\")\n",
    "\n",
    "        ens = []\n",
    "        for s in models.keys():\n",
    "            ens.append(predict(models[s], Xz[te_idx], device=dev, bs=max(8192, CFG.BATCH_SIZE)))\n",
    "        rb_hat[te_idx] = np.mean(np.vstack(ens), axis=0)\n",
    "        del loader; gc.collect()\n",
    "        if dev=='cuda': torch.cuda.empty_cache()\n",
    "\n",
    "    meta_eval = meta.copy()\n",
    "    meta_eval['rb_cnn_node'] = rb_hat\n",
    "    nnz = int(np.isfinite(meta_eval['rb_cnn_node']).sum())\n",
    "    print(f\"\\nOOS non-NaN preds: {nnz} / {len(meta_eval)}\")\n",
    "\n",
    "    print(f\"\\nBuilding decile tables (N={CFG.PORTS_N}) using rb_cnn_node ...\")\n",
    "    tbl_oiw = decile_table(meta_eval, 'rb_cnn_node', CFG.TARGET_COL, n_ports=CFG.PORTS_N, weighting='oiw')\n",
    "    tbl_ew  = decile_table(meta_eval, 'rb_cnn_node', CFG.TARGET_COL, n_ports=CFG.PORTS_N, weighting='ew')\n",
    "\n",
    "    print(\"\\n=== (a) Dollar open interest weighted ===\")\n",
    "    print(tbl_oiw.round({'Pred.':CFG.HOF_ROUND,'Real.':CFG.HOF_ROUND,'t':CFG.HOF_ROUND,'SR':CFG.HOF_ROUND}).to_string())\n",
    "    print(\"\\n=== (b) Equal weighted ===\")\n",
    "    print(tbl_ew.round({'Pred.':CFG.HOF_ROUND,'Real.':CFG.HOF_ROUND,'t':CFG.HOF_ROUND,'SR':CFG.HOF_ROUND}).to_string())\n",
    "\n",
    "\n",
    "    try_cols = [c for c in CFG.OPTION_CHAR_COLS if c in meta_eval.columns]\n",
    "    if len(try_cols)==0:\n",
    "        print(\"\\n[Info] No option-characteristic columns at meta.\")\n",
    "    else:\n",
    "        print(\"\\nBuilding table for OPTION characteristics\")\n",
    "        tbl_opt_oiw, pretty_oiw = hofler_option_chars_table(\n",
    "            meta_eval, pred_col='rb_cnn_node', char_cols=try_cols,\n",
    "            n_ports=CFG.PORTS_N, weighting='oiw', lags=CFG.LAGS_NW,\n",
    "            pct_cols=CFG.OPTION_CHAR_PCT, rnd=CFG.HOF_ROUND,\n",
    "            scalers=CFG.OPTION_CHAR_SCALERS  \n",
    "        )\n",
    "        tbl_opt_ew,  pretty_ew  = hofler_option_chars_table(\n",
    "            meta_eval, pred_col='rb_cnn_node', char_cols=try_cols,\n",
    "            n_ports=CFG.PORTS_N, weighting='ew',  lags=CFG.LAGS_NW,\n",
    "            pct_cols=CFG.OPTION_CHAR_PCT, rnd=CFG.HOF_ROUND,\n",
    "            scalers=CFG.OPTION_CHAR_SCALERS  \n",
    "        )\n",
    "        print(\"\\n=== (c) Option characteristics — OIW ===\")\n",
    "        print(pretty_oiw.to_string())\n",
    "        print(\"\\n=== (d) Option characteristics — EW ===\")\n",
    "        print(pretty_ew.to_string())\n",
    "\n",
    "    #shap analysis\n",
    "    if CFG.SHAP_ENABLE:\n",
    "        print(\"\\nRunning SHAP analysis (Deep/GradientExplainer) on ensemble...\")\n",
    "        run_shap_analysis(Xz, meta_eval, models, first_oos, device=dev)\n",
    "\n",
    "    try:\n",
    "        if getattr(CFG, \"LIME_ENABLE\", True):\n",
    "            print(\"\\nRunning LIME analysis on selected OOS examples...\")\n",
    "            run_lime_analysis(Xz, meta_eval, models, first_oos, device=dev)\n",
    "    except Exception as e:\n",
    "        print(f\"LIME analysis skipped/failed: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    return meta_eval, tbl_oiw, tbl_ew\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    meta_eval, tbl_oiw, tbl_ew = main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
