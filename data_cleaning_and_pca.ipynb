{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a4168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wrds\n",
    "from datetime import date, timedelta\n",
    "import re\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import numpy as np\n",
    "import math\n",
    "from numba import njit\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "#google colab libraries\n",
    "# import os\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9750aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = pd.read_parquet(\"/content/drive/MyDrive/Dissertation/data/ff_factors_2005_2022.parquet\") #ken french risk free is available on github\n",
    "db =wrds.Connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1996f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKERS_FILE = \"/content/drive/MyDrive/Dissertation/sp500_tickers_all_2000_2023.txt\"\n",
    "\n",
    "#import options\n",
    "START, END = pd.Timestamp(\"2005-01-01\"), pd.Timestamp(\"2022-12-31\")\n",
    "CHUNK = 1000\n",
    "USE_ATM_SELECTION = True\n",
    "MIN_PRICE = 5.0\n",
    "OP_LIB  = \"optionm\"         \n",
    "ALL_LIB = \"optionm_all\"\n",
    "\n",
    "MIN_ST_PRICE = 5.0\n",
    "\n",
    "def third_friday(y, m):\n",
    "    d = date(y, m, 15)\n",
    "    while d.weekday() != 4:  # Friday\n",
    "        d += timedelta(days=1)\n",
    "    return d\n",
    "\n",
    "def next_bday(d):\n",
    "    d = d + timedelta(days=1)\n",
    "    while d.weekday() >= 5:  # skip Sat/Sun\n",
    "        d += timedelta(days=1)\n",
    "    return d\n",
    "\n",
    "def table_cols(lib, tbl):\n",
    "    desc = db.describe_table(lib, tbl)\n",
    "    col_field = 'name' if 'name' in desc.columns else desc.columns[0]\n",
    "    return [str(x).lower() for x in desc[col_field].tolist()]\n",
    "\n",
    "def pick(cols, candidates):\n",
    "    for c in candidates:\n",
    "        if c in cols: return c\n",
    "\n",
    "    for c in candidates:\n",
    "        for col in cols:\n",
    "            if c in col: return col\n",
    "    return None\n",
    "\n",
    "def chunks(lst, n=800):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "tickers = [t.strip().upper() for t in open(TICKERS_FILE) if t.strip()]\n",
    "tickers = sorted(set(tickers))\n",
    "\n",
    "#map the tickers with the secid\n",
    "def find_symbol_map(tickers):\n",
    "    cands = []\n",
    "    for t in db.list_tables(OP_LIB):\n",
    "        tl = t.lower()\n",
    "        if tl in {\"security\",\"secnmd\"} or (\"sec\" in tl and any(x in tl for x in [\"name\",\"nmd\",\"nme\"])):\n",
    "            cands.append(t)\n",
    "    if not cands:\n",
    "        raise SystemExit(\"No mapping table found (optionm.security / optionm.secnmd).\")\n",
    "\n",
    "    out_rows = []\n",
    "    for tbl in cands:\n",
    "        cols = table_cols(OP_LIB, tbl)\n",
    "        id_col   = pick(cols, [\"secid\",\"securityid\",\"sec_id\"])\n",
    "        sym_col  = pick(cols, [\"symbol\",\"ticker\",\"sec_symbol\",\"issue_symbol\",\"root\",\"sym_root\"])\n",
    "        from_col = pick(cols, [\"effdt\",\"startdate\",\"start_date\",\"from\",\"fromdate\",\"startdt\",\"effective_from\"])\n",
    "        to_col   = pick(cols, [\"enddt\",\"end_date\",\"thru\",\"thrudate\",\"effective_to\",\"to\",\"enddt\"])\n",
    "        if (id_col is None) or (sym_col is None):\n",
    "            continue\n",
    "\n",
    "        pull_cols = [id_col, sym_col] + ([from_col] if from_col else []) + ([to_col] if to_col else [])\n",
    "        found = []\n",
    "        for tk_chunk in chunks(tickers, 300):\n",
    "            syms = \",\".join([\"'%s'\" % s.replace(\"'\", \"''\") for s in tk_chunk])\n",
    "            sql = f\"SELECT {', '.join(pull_cols)} FROM {OP_LIB}.{tbl} WHERE upper({sym_col}) IN ({syms})\"\n",
    "            df = db.raw_sql(sql)\n",
    "            if not df.empty:\n",
    "                df.columns = [c.lower() for c in df.columns]\n",
    "                df.rename(columns={id_col.lower():\"secid\", sym_col.lower():\"ticker\"}, inplace=True)\n",
    "                df[\"ticker\"] = df[\"ticker\"].astype(str).str.upper()\n",
    "                if from_col: df.rename(columns={from_col.lower():\"start_eff\"}, inplace=True)\n",
    "                if to_col:   df.rename(columns={to_col.lower():\"end_eff\"},   inplace=True)\n",
    "                keep_cols = [\"secid\",\"ticker\"] + ([\"start_eff\"] if from_col else []) + ([\"end_eff\"] if to_col else [])\n",
    "                found.append(df[keep_cols])\n",
    "\n",
    "        if found:\n",
    "            m = pd.concat(found, ignore_index=True).drop_duplicates()\n",
    "            out_rows.append(m)\n",
    "\n",
    "    if not out_rows:\n",
    "        raise SystemExit(\"Could not map any tickers to secid (check your tickers or OptionMetrics tables).\")\n",
    "\n",
    "    m = pd.concat(out_rows, ignore_index=True).drop_duplicates()\n",
    "\n",
    "    if {\"start_eff\",\"end_eff\"}.issubset(m.columns):\n",
    "        for c in [\"start_eff\",\"end_eff\"]:\n",
    "            m[c] = pd.to_datetime(m[c], errors=\"coerce\")\n",
    "        mask = (\n",
    "            (m[\"start_eff\"].isna() | (m[\"start_eff\"] <= END)) &\n",
    "            (m[\"end_eff\"].isna()   | (m[\"end_eff\"] >= START))\n",
    "        )\n",
    "        m = m.loc[mask]\n",
    "\n",
    "    m = m[[\"secid\",\"ticker\"]].dropna().drop_duplicates()\n",
    "    m[\"secid\"] = m[\"secid\"].astype(int)\n",
    "    return m\n",
    "\n",
    "sec_map = find_symbol_map(tickers)\n",
    "secids = sorted(sec_map[\"secid\"].unique())\n",
    "print(f\"[INFO] Mapped {len(secids)} secids for {sec_map['ticker'].nunique()} tickers.\")\n",
    "if len(secids) == 0:\n",
    "    raise SystemExit(\"Mapping returned 0 secids. Stop.\")\n",
    "\n",
    "#monthly schedule\n",
    "months = pd.period_range(START, END, freq=\"M\")\n",
    "schedule = pd.DataFrame({\n",
    "    \"month\": months.to_timestamp(how=\"start\"),\n",
    "    \"formation_date\": [pd.Timestamp(next_bday(third_friday(p.year,p.month))) for p in months],\n",
    "    \"next_exp_friday\": [pd.Timestamp(third_friday((p+1).year,(p+1).month)) for p in months],\n",
    "})\n",
    "schedule[\"next_exp_saturday\"] = schedule[\"next_exp_friday\"] + pd.Timedelta(days=1)\n",
    "\n",
    "op_tbls = set(t.lower() for t in db.list_tables(OP_LIB))\n",
    "OP_BY_YEAR = {}\n",
    "for t in op_tbls:\n",
    "    m = re.fullmatch(r\"opprcd(\\d{4})\", t)\n",
    "    if m:\n",
    "        OP_BY_YEAR[int(m.group(1))] = t\n",
    "if not OP_BY_YEAR:\n",
    "    raise SystemExit(\"No opprcdYYYY tables in optionm.*\")\n",
    "\n",
    "probe_year = max(min(OP_BY_YEAR), min(max(OP_BY_YEAR), START.year))\n",
    "op_cols = table_cols(OP_LIB, OP_BY_YEAR[probe_year])\n",
    "OP_STRIKE = pick(op_cols, [\"strike_price\",\"strike\"])\n",
    "OP_BID    = pick(op_cols, [\"best_bid\",\"bid\"])\n",
    "OP_ASK    = pick(op_cols, [\"best_offer\",\"ask\"])\n",
    "OP_IV     = pick(op_cols, [\"impl_volatility\",\"implied_volatility\",\"iv\"])\n",
    "for name,val in [(\"OP_STRIKE\",OP_STRIKE),(\"OP_BID\",OP_BID),(\"OP_ASK\",OP_ASK),(\"OP_IV\",OP_IV)]:\n",
    "    if val is None:\n",
    "        raise SystemExit(f\"Missing column in opprcdYYYY: {name}\")\n",
    "    \n",
    "if USE_ATM_SELECTION:\n",
    "    all_tbls = set(t.lower() for t in db.list_tables(ALL_LIB))\n",
    "    SECPRD_UNIFIED = \"secprd\" in all_tbls\n",
    "    sc_cols = table_cols(ALL_LIB,\"secprd\") if SECPRD_UNIFIED else table_cols(OP_LIB, f\"secprd{probe_year}\")\n",
    "    SC_CLOSE = pick(sc_cols, [\"close\",\"close_price\",\"closing_price\",\"prc\",\"price\"])\n",
    "    if SC_CLOSE is None:\n",
    "        raise SystemExit(f\"Missing close price column in secprd: {sc_cols}\")\n",
    "\n",
    "\n",
    "#extract the data \n",
    "rows, month_stats = [], []\n",
    "for _, r in schedule.iterrows():\n",
    "    form_dt = r['formation_date'].date()\n",
    "    exp_fri = r['next_exp_friday'].date()\n",
    "    exp_sat = r['next_exp_saturday'].date()\n",
    "    exdate_list = [exp_fri, exp_sat]\n",
    "\n",
    "    \n",
    "    probe_dt = form_dt\n",
    "    pulled = None\n",
    "    for _ in range(5):\n",
    "        y = pd.Timestamp(probe_dt).year\n",
    "        op_tbl = OP_BY_YEAR.get(y)\n",
    "        if op_tbl:\n",
    "            parts = []\n",
    "            for sec_chunk in chunks(secids, CHUNK):\n",
    "                sql = f\"\"\"\n",
    "                  SELECT secid, date, exdate, cp_flag,\n",
    "                         {OP_STRIKE} AS k,\n",
    "                         {OP_BID}    AS bid,\n",
    "                         {OP_ASK}    AS ask,\n",
    "                         volume, open_interest,\n",
    "                         {OP_IV}     AS iv,\n",
    "                         delta, gamma, vega, theta\n",
    "                  FROM {OP_LIB}.{op_tbl}\n",
    "                  WHERE cp_flag = 'C'\n",
    "                    AND date    = '{probe_dt}'\n",
    "                    AND exdate IN ('{exdate_list[0]}','{exdate_list[1]}')\n",
    "                    AND secid IN ({\",\".join(map(str,sec_chunk))})\n",
    "                \"\"\"\n",
    "                df = db.raw_sql(sql)\n",
    "                if not df.empty:\n",
    "                    df.columns = [c.lower() for c in df.columns]\n",
    "                    parts.append(df)\n",
    "            if parts:\n",
    "                pulled = pd.concat(parts, ignore_index=True)\n",
    "                pulled['date']   = pd.to_datetime(pulled['date'],   errors='coerce').dt.normalize()\n",
    "                pulled['exdate'] = pd.to_datetime(pulled['exdate'], errors='coerce').dt.normalize()\n",
    "                pulled['secid']  = pulled['secid'].astype(int)\n",
    "                break\n",
    "        probe_dt = (pd.Timestamp(probe_dt) + pd.Timedelta(days=1)).date()\n",
    "\n",
    "    if pulled is None or pulled.empty:\n",
    "        month_stats.append((form_dt, 0, 0))\n",
    "        continue\n",
    "\n",
    "    pulled = pulled.merge(sec_map, on=\"secid\", how=\"left\")\n",
    "    pre = len(pulled)   \n",
    "\n",
    "\n",
    "    if USE_ATM_SELECTION:\n",
    "        pulled['moneyness'] = pulled['s_t'] / pulled['k']\n",
    "        pulled = pulled[(pulled['moneyness']>=0.8) & (pulled['moneyness']<=1.2)]\n",
    "        if pulled.empty:\n",
    "            month_stats.append((probe_dt, pre, 0)); continue\n",
    "        pulled['dist'] = (pulled['moneyness'] - 1.0).abs()\n",
    "        pulled['is_otm_pref'] = (pulled['moneyness'] <= 1.0).astype(int)\n",
    "        pulled = (pulled.sort_values(['secid','dist','is_otm_pref'], ascending=[True,True,False])\n",
    "                        .groupby('secid', as_index=False).head(1))\n",
    "\n",
    "    pulled['formation_date'] = pd.to_datetime(pd.Timestamp(probe_dt).normalize())\n",
    "    pulled['next_expiry']    = pd.to_datetime(pulled['exdate'], errors='coerce').dt.normalize()\n",
    "    pulled['days_to_expiry'] = (pulled['next_expiry'] - pulled['formation_date']).dt.days\n",
    "\n",
    "    month_stats.append((probe_dt, pre, len(pulled)))\n",
    "    rows.append(pulled)\n",
    "\n",
    "out = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fd0bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "SP500_CSV = \"/content/drive/MyDrive/Dissertation/sp500_monthly_tickers_2005_2022.csv\" #monthly tickers on sp500 available on github\n",
    "sp = pd.read_csv(SP500_CSV)\n",
    "sp.columns = sp.columns.str.strip().str.lower()\n",
    "#formating the list\n",
    "sp['month'] = pd.to_datetime(sp['date'], errors='coerce').dt.to_period('M').dt.to_timestamp('D').dt.normalize()\n",
    "def _explode_tickers(cell: str):\n",
    "    s = str(cell)\n",
    "    s = re.sub(r'[\\[\\]\\{\\}\"\\']', '', s)     \n",
    "    s = re.sub(r'[,\\|;]+', ' ', s)          \n",
    "    s = re.sub(r'\\s+', ' ', s).strip()     \n",
    "    if not s:\n",
    "        return []\n",
    "    toks = s.split(' ')\n",
    "\n",
    "    toks = [re.sub(r'[.\\-]', '', t.upper()) for t in toks if t]\n",
    "    return toks\n",
    "\n",
    "sp['ticker'] = sp['tickers'].dropna().apply(_explode_tickers)\n",
    "sp = sp.explode('ticker')\n",
    "sp = sp[['month','ticker']].dropna().drop_duplicates()\n",
    "\n",
    "if 'formation_date' in out.columns:\n",
    "    form_col = 'formation_date'\n",
    "elif 'date' in out.columns:\n",
    "    form_col = 'date'\n",
    "else:\n",
    "    raise ValueError(\"Δεν βρίσκω ούτε 'formation_date' ούτε 'date' στο out.\")\n",
    "\n",
    "out = out.copy()\n",
    "out[form_col] = pd.to_datetime(out[form_col], errors='coerce')\n",
    "out['month']  = out[form_col].dt.to_period('M').dt.to_timestamp('D').dt.normalize()\n",
    "\n",
    "if 'ticker' not in out.columns:\n",
    "    raise ValueError(\"Το out δεν έχει στήλη 'ticker'. Δες το helper στο τέλος για να το προσθέσεις.\")\n",
    "\n",
    "out['ticker_norm'] = out['ticker'].astype(str).str.upper().str.replace(r'[.\\-]', '', regex=True)\n",
    "\n",
    "#filter and keep what matches\n",
    "sp['in_sp500'] = True\n",
    "keep = out.merge(sp.rename(columns={'ticker':'ticker_norm'}),\n",
    "                 on=['month','ticker_norm'], how='left')\n",
    "\n",
    "out_sp = keep.loc[keep['in_sp500'].fillna(False)].drop(columns=['in_sp500'])\n",
    "out_sp = out_sp.drop(columns=['ticker_norm'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b30e3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTS = out_sp.copy()                   \n",
    "OP_LIB, ALL_LIB = \"optionm\", \"optionm_all\"\n",
    "S0_BACKUP_DAYS = 4                      \n",
    "ST_WINDOW_DAYS = 7                     \n",
    "\n",
    "def table_cols(lib, tbl):\n",
    "    desc = db.describe_table(lib, tbl)\n",
    "    col_field = 'name' if 'name' in desc.columns else desc.columns[0]\n",
    "    return [str(x).lower() for x in desc[col_field].tolist()]\n",
    "\n",
    "all_tbls = set(t.lower() for t in db.list_tables(ALL_LIB))\n",
    "SECPRD_UNIFIED = \"secprd\" in all_tbls\n",
    "\n",
    "#find the correct name of the column fot the closing price\n",
    "probe_year = OPTS['formation_date'].min().year if len(OPTS) else 2005\n",
    "sc_cols = table_cols(ALL_LIB,\"secprd\") if SECPRD_UNIFIED else table_cols(OP_LIB, f\"secprd{probe_year}\")\n",
    "SC_CLOSE = next((c for c in [\"close\",\"close_price\",\"closing_price\",\"prc\",\"price\"] if c in sc_cols), None)\n",
    "if SC_CLOSE is None:\n",
    "    raise SystemExit(f\"Δεν βρέθηκε στήλη τιμής στο secprd. Columns: {sc_cols}\")\n",
    "\n",
    "OPTS['formation_date'] = pd.to_datetime(OPTS['formation_date']).dt.normalize()\n",
    "OPTS['next_expiry']    = pd.to_datetime(OPTS['next_expiry']).dt.normalize()\n",
    "OPTS['secid']          = OPTS['secid'].astype(int)\n",
    "\n",
    "#S0 on expiration\n",
    "def fetch_S0_for_date(d, secids):\n",
    "    secids_sql = \",\".join(map(str, secids))\n",
    "    if SECPRD_UNIFIED:\n",
    "        sql = f\"\"\"\n",
    "          SELECT secid, date, {SC_CLOSE} AS close\n",
    "          FROM {ALL_LIB}.secprd\n",
    "          WHERE secid IN ({secids_sql}) AND date BETWEEN '{(d - pd.Timedelta(days=S0_BACKUP_DAYS)).date()}'\n",
    "                                               AND '{d.date()}'\n",
    "        \"\"\"\n",
    "    else:\n",
    "        y0, y1 = (d - pd.Timedelta(days=S0_BACKUP_DAYS)).year, d.year\n",
    "        parts = []\n",
    "        for y in sorted({y0,y1}):\n",
    "            sql = f\"\"\"\n",
    "              SELECT secid, date, {SC_CLOSE} AS close\n",
    "              FROM {OP_LIB}.secprd{y}\n",
    "              WHERE secid IN ({secids_sql}) AND date BETWEEN '{(d - pd.Timedelta(days=S0_BACKUP_DAYS)).date()}'\n",
    "                                                   AND '{d.date()}'\n",
    "            \"\"\"\n",
    "            parts.append(db.raw_sql(sql))\n",
    "        df = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "        df.columns = [c.lower() for c in df.columns]\n",
    "        if not df.empty:\n",
    "            df['date'] = pd.to_datetime(df['date']).dt.normalize()\n",
    "        return (df.sort_values(['secid','date'])\n",
    "                  .groupby('secid', as_index=False).tail(1)  # max date ≤ d\n",
    "                  .rename(columns={'date':'s0_date','close':'S0'}))\n",
    "\n",
    "    df = db.raw_sql(sql)\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=['secid','s0_date','S0'])\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.normalize()\n",
    "    return (df.sort_values(['secid','date'])\n",
    "              .groupby('secid', as_index=False).tail(1)\n",
    "              .rename(columns={'date':'s0_date','close':'S0'}))\n",
    "\n",
    "#ST on the expiration day\n",
    "def fetch_ST_for_exdate(exd, secids):\n",
    "    secids_sql = \",\".join(map(str, secids))\n",
    "    start = (exd - pd.Timedelta(days=ST_WINDOW_DAYS)).date()\n",
    "    stop  = exd.date()\n",
    "    if SECPRD_UNIFIED:\n",
    "        sql = f\"\"\"\n",
    "          SELECT secid, date, {SC_CLOSE} AS close\n",
    "          FROM {ALL_LIB}.secprd\n",
    "          WHERE secid IN ({secids_sql}) AND date BETWEEN '{start}' AND '{stop}'\n",
    "        \"\"\"\n",
    "        df = db.raw_sql(sql)\n",
    "    else:\n",
    "        y0, y1 = (exd - pd.Timedelta(days=ST_WINDOW_DAYS)).year, exd.year\n",
    "        parts = []\n",
    "        for y in sorted({y0,y1}):\n",
    "            sql = f\"\"\"\n",
    "              SELECT secid, date, {SC_CLOSE} AS close\n",
    "              FROM {OP_LIB}.secprd{y}\n",
    "              WHERE secid IN ({secids_sql}) AND date BETWEEN '{start}' AND '{stop}'\n",
    "            \"\"\"\n",
    "            parts.append(db.raw_sql(sql))\n",
    "        df = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=['secid','st_date','ST'])\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.normalize()\n",
    "    return (df.sort_values(['secid','date'])\n",
    "              .groupby('secid', as_index=False).tail(1)   # max date ≤ exdate\n",
    "              .rename(columns={'date':'st_date','close':'ST'}))\n",
    "\n",
    "OPTS = OPTS.sort_values('formation_date')\n",
    "s0_rows, st_rows = [], []\n",
    "\n",
    "for d, dfm in OPTS.groupby('formation_date'):\n",
    "    secids = sorted(dfm['secid'].unique().tolist())\n",
    "    if not secids:\n",
    "        continue\n",
    "    s0 = fetch_S0_for_date(d, secids)\n",
    "    s0['formation_date'] = d\n",
    "    s0_rows.append(s0)\n",
    "\n",
    "for exd, dfe in OPTS.groupby('next_expiry'):\n",
    "    secids = sorted(dfe['secid'].unique().tolist())\n",
    "    if not secids:\n",
    "        continue\n",
    "    st = fetch_ST_for_exdate(exd, secids)\n",
    "    st['next_expiry'] = exd\n",
    "    st_rows.append(st)\n",
    "\n",
    "S0_all = pd.concat(s0_rows, ignore_index=True) if s0_rows else pd.DataFrame(columns=['secid','s0_date','S0','formation_date'])\n",
    "ST_all = pd.concat(st_rows, ignore_index=True) if st_rows else pd.DataFrame(columns=['secid','st_date','ST','next_expiry'])\n",
    "\n",
    "#merges\n",
    "OPTS2 = OPTS.merge(S0_all[['secid','formation_date','S0','s0_date']], on=['secid','formation_date'], how='left')\n",
    "OPTS2 = OPTS2.merge(ST_all[['secid','next_expiry','ST','st_date']], on=['secid','next_expiry'],   how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7105af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = OPTS2.copy()\n",
    "\n",
    "for c in ['date','exdate','formation_date','next_expiry','s0_date','st_date']:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_datetime(df[c], errors='coerce').dt.normalize()\n",
    "#keep only the unique values drop the duplicates\n",
    "if {'formation_date','date'}.issubset(df.columns) and (df['formation_date'].fillna(df['date'])==df['date']).all():\n",
    "    df = df.drop(columns=['formation_date'])\n",
    "if {'next_expiry','exdate'}.issubset(df.columns) and (df['next_expiry'].fillna(df['exdate'])==df['exdate']).all():\n",
    "    df = df.drop(columns=['next_expiry'])\n",
    "\n",
    "df['end_date'] = (df['st_date'] if 'st_date' in df.columns else df['exdate'])\n",
    "if 'st_date' in df.columns:\n",
    "    df['end_date'] = df['st_date'].where(df['st_date'].notna(), df['exdate'])\n",
    "\n",
    "#we do not keep if it has not end_date\n",
    "df = df.dropna(subset=['date','end_date']).copy()\n",
    "df['days_to_expiry'] = (df['end_date'] - df['date']).dt.days\n",
    "\n",
    "#we adapt the ken french risk free\n",
    "ff2 = ff.copy()\n",
    "ff2 = ff2.reset_index().rename(columns={'index':'date'}) if 'date' not in ff2.columns else ff2.reset_index()\n",
    "ff2['date'] = pd.to_datetime(ff2['date'], errors='coerce').dt.normalize()\n",
    "\n",
    "ff2['rf_daily']  = pd.to_numeric(ff2['RF'], errors='coerce') / 100.0\n",
    "ff2['rf_annual'] = (1.0 + ff2['rf_daily'])**365 - 1.0\n",
    "ff2 = ff2.sort_values('date')\n",
    "ff2['cum_log1p'] = np.log1p(ff2['rf_daily']).cumsum()\n",
    "\n",
    "ff_small = ff2[['date','rf_daily','rf_annual','cum_log1p']].copy()\n",
    "\n",
    "# r_t at the formationd ay\n",
    "df = df.sort_values('date')\n",
    "df = pd.merge_asof(\n",
    "    df,\n",
    "    ff_small[['date','rf_daily','rf_annual','cum_log1p']].rename(\n",
    "        columns={'rf_daily':'rf_daily_t','rf_annual':'rf_annual_t','cum_log1p':'cum_t'}\n",
    "    ),\n",
    "    on='date', direction='backward'\n",
    ")\n",
    "\n",
    "# r_T at the end date\n",
    "right_T = ff_small[['date','cum_log1p']].rename(columns={'date':'end_date','cum_log1p':'cum_T'})\n",
    "df = df.sort_values('end_date')\n",
    "df = pd.merge_asof(df, right_T, on='end_date', direction='backward')\n",
    "#annualized * fraction\n",
    "df['rf_simple_period'] = df['rf_annual_t'] * (df['days_to_expiry'].astype(float)/365.0)\n",
    "\n",
    "df['rf_compounded_period'] = np.expm1(df['cum_T'] - df['cum_t'])\n",
    "\n",
    "opts2_ready = df\n",
    "print(opts2_ready[['date','end_date','days_to_expiry','rf_daily_t','rf_annual_t','rf_simple_period','rf_compounded_period']].head())\n",
    "opts2_ready['k'] = opts2_ready['k'] / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2939109",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_WRDS = True             \n",
    "IV_COL   = 'iv'              \n",
    "DAY_MULT_BASE = 3            #steps per calendar day\n",
    "USE_RICHARDSON = True        \n",
    "\n",
    "#depends if you work on colab or not\n",
    "try:\n",
    "    from scipy.special import erf\n",
    "except Exception:\n",
    "    from numpy import erf\n",
    "#binomial tree for finding the delta manual\n",
    "@njit(fastmath=True)\n",
    "def _phi(x):\n",
    "    t = 1.0/(1.0+0.2316419*abs(x))\n",
    "    y = 1.0 - (0.319381530*t - 0.356563782*t**2 + 1.781477937*t**3 - 1.821255978*t**4 + 1.330274429*t**5) \\\n",
    "              * math.exp(-0.5*x*x)/math.sqrt(2*math.pi)\n",
    "    return y if x>=0 else 1.0-y\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def bs_call_price(S, K, r, sigma, T):\n",
    "    if T<=0:   return max(S-K, 0.0)\n",
    "    if sigma<=0: return max(S - K*math.exp(-r*T), 0.0)\n",
    "    srt = sigma*math.sqrt(T)\n",
    "    d1  = (math.log(S/K) + (r + 0.5*sigma*sigma)*T)/srt\n",
    "    d2  = d1 - srt\n",
    "    return S*_phi(d1) - K*math.exp(-r*T)*_phi(d2)\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def bs_call_delta(S, K, r, sigma, T):\n",
    "    if T<=0 or sigma<=0:\n",
    "        return 1.0 if S>K else (0.0 if S<K else 0.5)\n",
    "    srt = sigma*math.sqrt(T)\n",
    "    d1  = (math.log(S/K) + (r + 0.5*sigma*sigma)*T)/srt\n",
    "    return _phi(d1)\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def _interp1(x, xp, fp):\n",
    "    n = xp.shape[0]\n",
    "    if x <= xp[0]:   return fp[0]\n",
    "    if x >= xp[n-1]: return fp[n-1]\n",
    "    lo, hi = 0, n-1\n",
    "    while hi - lo > 1:\n",
    "        mid = (lo+hi)//2\n",
    "        if xp[mid] <= x: lo = mid\n",
    "        else:            hi = mid\n",
    "    x0, x1 = xp[lo], xp[hi]\n",
    "    y0, y1 = fp[lo], fp[hi]\n",
    "    return y0 + (y1-y0)*(x - x0)/(x1 - x0)\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def _interp_vec(xs, xp, fp, out):\n",
    "    for i in range(xs.shape[0]):\n",
    "        out[i] = _interp1(xs[i], xp, fp)\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def _american_binomial_core(S0, K, r, sigma, T, is_call, t_div, D_div, N):\n",
    "    if N < 120: N = 120\n",
    "    if T <= 0.0:\n",
    "        if is_call:\n",
    "            return max(S0-K, 0.0), (1.0 if S0>K else (0.5 if S0==K else 0.0))\n",
    "        else:\n",
    "            return max(K-S0, 0.0), (-1.0 if S0<K else (-0.5 if S0==K else 0.0))\n",
    "\n",
    "    dt   = T / N\n",
    "    disc = math.exp(-r*dt)\n",
    "    u    = math.exp(sigma*math.sqrt(dt))\n",
    "    d    = 1.0/u\n",
    "    m    = math.exp(r*dt)\n",
    "    p    = (m - d) / (u - d)\n",
    "    if p<0.0: p=0.0\n",
    "    if p>1.0: p=1.0\n",
    "\n",
    "    Sj = np.empty(N+1)\n",
    "    for i in range(N+1):\n",
    "        Sj[i] = S0 * (u**i) * (d**(N-i))\n",
    "    Vn = np.maximum(Sj - K, 0.0) if is_call else np.maximum(K - Sj, 0.0)\n",
    "\n",
    "    div_by_step = np.zeros(N+1)\n",
    "    if t_div is not None and t_div.shape[0] > 0:\n",
    "        for k in range(t_div.shape[0]):\n",
    "            j = int(round(t_div[k]/dt))\n",
    "            if j>0 and j<=N:\n",
    "                div_by_step[j] += D_div[k]\n",
    "\n",
    "    delta_root = 0.0\n",
    "    for j in range(N-1, -1, -1):\n",
    "        Sj  = np.empty(j+1)\n",
    "        Sj1 = np.empty(j+2)\n",
    "        base  = S0 * (d**j);  ratio = (u/d)\n",
    "        for i in range(j+1):\n",
    "            Sj[i] = base * (ratio**i)\n",
    "        base1 = S0 * (d**(j+1))\n",
    "        for i in range(j+2):\n",
    "            Sj1[i] = base1 * (ratio**i)\n",
    "\n",
    "        Vuse = Vn\n",
    "        if div_by_step[j+1] > 0.0:\n",
    "            D = div_by_step[j+1]\n",
    "            Sshift = Sj1 - D\n",
    "            tmp = np.empty(Sj1.shape[0])\n",
    "            _interp_vec(Sshift, Sj1, Vn, tmp)\n",
    "            payoff = np.maximum(Sj1 - K, 0.0) if is_call else np.maximum(K - Sj1, 0.0)\n",
    "            Vuse = np.maximum(payoff, tmp)\n",
    "\n",
    "        V = disc*(p*Vuse[1:] + (1.0-p)*Vuse[:-1])\n",
    "        exer = np.maximum(Sj - K, 0.0) if is_call else np.maximum(K - Sj, 0.0)\n",
    "        Vcurr = np.maximum(exer, V)\n",
    "\n",
    "        if j == 0:\n",
    "            denom = (Sj1[1] - Sj1[0])\n",
    "            delta_root = (Vuse[1] - Vuse[0]) / denom if denom!=0.0 else 0.0\n",
    "\n",
    "        Vn = Vcurr\n",
    "\n",
    "    return float(Vn[0]), float(delta_root)\n",
    "\n",
    "def american_price_delta(S0, K, r, sigma, T, is_call, div_schedule, day_mult=DAY_MULT_BASE):\n",
    "    days = max(1, int(round(T*365.0)))\n",
    "    N = max(120, days * int(day_mult))\n",
    "    if div_schedule and len(div_schedule)>0:\n",
    "        t_div = np.array([float(t) for (t, D) in div_schedule], dtype=np.float64)\n",
    "        D_div = np.array([float(D) for (t, D) in div_schedule], dtype=np.float64)\n",
    "    else:\n",
    "        t_div = np.empty(0, dtype=np.float64)\n",
    "        D_div = np.empty(0, dtype=np.float64)\n",
    "    return _american_binomial_core(float(S0),float(K),float(r),float(sigma),float(T),bool(is_call),\n",
    "                                   t_div, D_div, int(N))\n",
    "\n",
    "def american_price_delta_extrap(S0, K, r, sigma, T, is_call, div_schedule, day_mult=DAY_MULT_BASE):\n",
    "    p1, d1 = american_price_delta(S0, K, r, sigma, T, is_call, div_schedule, day_mult=day_mult)\n",
    "    p2, d2 = american_price_delta(S0, K, r, sigma, T, is_call, div_schedule, day_mult=2*day_mult)\n",
    "    pX, dX = 2.0*p2 - p1, 2.0*d2 - d1\n",
    "    intr = max(S0-K,0.0) if is_call else max(K-S0,0.0)\n",
    "    pX = max(pX, intr)\n",
    "    if is_call:  dX = max(0.0, min(1.0, dX))\n",
    "    else:        dX = max(-1.0, min(0.0, dX))\n",
    "    return pX, dX\n",
    "\n",
    "#pull dividends\n",
    "def build_div_schedules_pair_from_wrds(df_opts, verbose=True):\n",
    "    import wrds, pandas as pd\n",
    "    db = wrds.Connection()\n",
    "\n",
    "    def _cols(db, lib, tbl):\n",
    "        q = f\"\"\"\n",
    "        select lower(column_name) as name\n",
    "        from information_schema.columns\n",
    "        where table_schema='{lib}' and table_name='{tbl}'\n",
    "        \"\"\"\n",
    "        return db.raw_sql(q)['name'].tolist()\n",
    "\n",
    "    def _pick(cols, cands):\n",
    "        for c in cands:\n",
    "            if c in cols: return c\n",
    "        for c in cands:\n",
    "            for col in cols:\n",
    "                if c in col: return col\n",
    "        return None\n",
    "\n",
    "    def _sched(series_df):\n",
    "        if series_df.empty:\n",
    "            return pd.Series(dtype=object)\n",
    "        g = (series_df[['row_id','t_i','amount']]\n",
    "             .dropna()\n",
    "             .sort_values(['row_id','t_i']))\n",
    "        return g.groupby('row_id').apply(\n",
    "            lambda x: list(zip(x['t_i'].astype(float).tolist(),\n",
    "                               x['amount'].astype(float).tolist()))\n",
    "        )\n",
    "\n",
    "    for tbl in ['distrd', 'distribution']:\n",
    "        cols = _cols(db, 'optionm', tbl)\n",
    "        sec = _pick(cols, ['secid','securityid','sec_id'])\n",
    "        exd = _pick(cols, ['exdate','ex_date','exdt','exdivdate'])\n",
    "        amt = _pick(cols, ['amount','divamt','cash_amount','div_amount','amt','amnt','cash'])\n",
    "        ann = _pick(cols, ['announcedate','anndate','declare_date','declarationdate','announce_date'])\n",
    "        if not (sec and exd and amt):\n",
    "            continue\n",
    "\n",
    "        ids = \",\".join(map(str, sorted(df_opts['secid'].astype(int).unique().tolist())))\n",
    "        s, e = df_opts['date'].min().date(), df_opts['exdate'].max().date()\n",
    "        sql = (\n",
    "            f\"select {sec} as secid, {exd} as div_exdate, {amt} as amount\" +\n",
    "            (f\", {ann} as announcedate\" if ann else \"\") +\n",
    "            f\" from optionm.{tbl} where {sec} in ({ids}) and {exd} between '{s}' and '{e}' and {amt} > 0\"\n",
    "        )\n",
    "        raw = db.raw_sql(sql)\n",
    "        if raw.empty:\n",
    "            continue\n",
    "\n",
    "        raw.columns = [c.lower() for c in raw.columns]\n",
    "        raw['div_exdate'] = pd.to_datetime(raw['div_exdate'])\n",
    "        if 'announcedate' in raw.columns:\n",
    "            raw['announcedate'] = pd.to_datetime(raw['announcedate'])\n",
    "\n",
    "        X = df_opts[['row_id','secid','date','exdate']].merge(raw, on='secid', how='left')\n",
    "        m = (X['div_exdate'] > X['date']) & (X['div_exdate'] <= X['exdate'])\n",
    "        X = X[m].copy()\n",
    "        X['t_i'] = (X['div_exdate'] - X['date']).dt.days / 365.0\n",
    "        if 'announcedate' in X.columns:\n",
    "            X['is_ann'] = X['announcedate'].notna() & (X['announcedate'] <= X['date'])\n",
    "        else:\n",
    "            X['is_ann'] = True\n",
    "\n",
    "        sched_A = _sched(X[X['is_ann']]).rename('div_schedule_ann')\n",
    "        sched_B = _sched(X).rename('div_schedule_expost')\n",
    "        return sched_A, sched_B\n",
    "\n",
    "\n",
    "    return pd.Series(dtype=object), pd.Series(dtype=object)\n",
    "\n",
    "\n",
    "def compute_deltas_on_df(DF_IN, iv_col='iv', use_wrds=False):\n",
    "    import math, numpy as np, pandas as pd\n",
    "\n",
    "    df = DF_IN.copy()\n",
    "\n",
    "    df['date']   = pd.to_datetime(df['date']).dt.normalize()\n",
    "    df['exdate'] = pd.to_datetime(df['exdate']).dt.normalize()\n",
    "    if 'cp_flag' not in df.columns:\n",
    "        df['cp_flag'] = 'C'  # όλα calls\n",
    "    if 'k' in df.columns and 'K' not in df.columns:\n",
    "        df = df.rename(columns={'k':'K'})\n",
    "    if 'days_to_expiry' in df.columns and 'T' not in df.columns:\n",
    "        df['T'] = df['days_to_expiry'].astype('float64')/365.0\n",
    "    if 'T' not in df.columns:\n",
    "        df['T'] = (df['exdate'] - df['date']).dt.days.astype('float64')/365.0\n",
    "\n",
    "\n",
    "    if 'r_eff' not in df.columns:\n",
    "        if {'rf_compounded_period','T'}.issubset(df.columns):\n",
    "            df['r_eff'] = np.where(df['T']>0,\n",
    "                                   np.log1p(pd.to_numeric(df['rf_compounded_period'], errors='coerce'))/df['T'],\n",
    "                                   0.0)\n",
    "        elif 'rf_annual_t' in df.columns:\n",
    "            df['r_eff'] = pd.to_numeric(df['rf_annual_t'], errors='coerce')\n",
    "        else:\n",
    "            df['r_eff'] = 0.0\n",
    "\n",
    "\n",
    "    if use_wrds and (('div_schedule_ann' not in df.columns) or ('div_schedule_expost' not in df.columns)):\n",
    "        df = df.reset_index(drop=True)\n",
    "        df['row_id'] = np.arange(len(df))\n",
    "        A, B = build_div_schedules_pair_from_wrds(df)\n",
    "        if not A.empty: df = df.join(A, on='row_id').join(B, on='row_id')\n",
    "    if 'div_schedule_ann' not in df.columns:    df['div_schedule_ann']    = [[] for _ in range(len(df))]\n",
    "    if 'div_schedule_expost' not in df.columns: df['div_schedule_expost'] = [[] for _ in range(len(df))]\n",
    "\n",
    "    for c in ['S0','K','r_eff','T', iv_col]:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce').astype('float64')\n",
    "\n",
    "    valid = df['T'].gt(0) & df[iv_col].gt(0) \\\n",
    "            & df[['S0','K','r_eff','T',iv_col]].apply(np.isfinite).all(axis=1)\n",
    "\n",
    "    def sf(x):\n",
    "        try:    return float(x)\n",
    "        except: return np.nan\n",
    "\n",
    "    def _row_pricer(r):\n",
    "        S0, K, T, rf, sigma = sf(r['S0']), sf(r['K']), sf(r['T']), sf(r['r_eff']), sf(r[iv_col])\n",
    "        if not (np.isfinite(S0) and np.isfinite(K) and np.isfinite(T) and np.isfinite(rf) and np.isfinite(sigma)):\n",
    "            return pd.Series({'price_am': np.nan, 'delta_model': np.nan})\n",
    "        if T<=0.0 or sigma<=0.0:\n",
    "            return pd.Series({'price_am': np.nan, 'delta_model': np.nan})\n",
    "\n",
    "        is_call = (r['cp_flag']=='C')\n",
    "        sched   = r['div_schedule_ann'] if isinstance(r['div_schedule_ann'], list) else []\n",
    "\n",
    "        if is_call and len(sched)==0:\n",
    "            return pd.Series({\n",
    "                'price_am': bs_call_price(S0, K, rf, sigma, T),\n",
    "                'delta_model': bs_call_delta(S0, K, rf, sigma, T)\n",
    "            })\n",
    "        p, d = american_price_delta_extrap(S0, K, rf, sigma, T, bool(is_call), sched,\n",
    "                                           day_mult=DAY_MULT_BASE)\n",
    "        return pd.Series({'price_am': p, 'delta_model': d})\n",
    "\n",
    "    out = pd.DataFrame(index=df.index, columns=['price_am','delta_model'], dtype='float64')\n",
    "    try:\n",
    "        from tqdm.auto import tqdm; tqdm.pandas()\n",
    "        out.loc[valid] = df.loc[valid].progress_apply(_row_pricer, axis=1)\n",
    "    except Exception:\n",
    "        out.loc[valid] = df.loc[valid].apply(_row_pricer, axis=1)\n",
    "\n",
    "    df = pd.concat([df, out], axis=1)\n",
    "\n",
    "    def _pv_div(sched, r, T):\n",
    "        if not isinstance(sched, list) or len(sched)==0 or not np.isfinite(r) or not np.isfinite(T):\n",
    "            return 0.0\n",
    "        return float(sum(float(D)*math.exp(-float(r)*float(t)) for (t, D) in sched))\n",
    "    df['pv_div_ann'] = [ _pv_div(v, rr, TT) for v, rr, TT in zip(df['div_schedule_ann'], df['r_eff'], df['T']) ]\n",
    "\n",
    "    def _bs_delta_vec(S, K, r, sig, T, is_call):\n",
    "        S = np.asarray(S, float); K=np.asarray(K, float); r=np.asarray(r, float)\n",
    "        sig=np.asarray(sig, float); T=np.asarray(T, float); is_call=np.asarray(is_call, bool)\n",
    "        out = np.full_like(S, np.nan, float)\n",
    "        ok = (T>0) & (sig>0) & np.isfinite(S) & np.isfinite(K) & np.isfinite(r)\n",
    "        if ok.any():\n",
    "            srt = sig[ok]*np.sqrt(T[ok])\n",
    "            d1  = (np.log(S[ok]/K[ok]) + (r[ok] + 0.5*sig[ok]**2)*T[ok]) / srt\n",
    "            phi = 0.5*(1.0 + erf(d1/np.sqrt(2.0)))\n",
    "            out[ok] = np.where(is_call[ok], phi, phi - 1.0)\n",
    "        bd = ~ok & np.isfinite(S) & np.isfinite(K)\n",
    "        if bd.any():\n",
    "            oc = is_call[bd]; SgK = S[bd]>K[bd]; SlK = S[bd]<K[bd]; eq = ~(SgK|SlK)\n",
    "            tmp = np.empty(bd.sum(), float)\n",
    "            tmp[ oc & SgK] =  1.0; tmp[ oc & SlK] =  0.0; tmp[ oc & eq] =  0.5\n",
    "            tmp[~oc & SlK] = -1.0; tmp[~oc & SgK] =  0.0; tmp[~oc & eq] = -0.5\n",
    "            out[bd] = tmp\n",
    "        return out\n",
    "\n",
    "    def _bs_delta_div_vec(S,K,r,sig,T,pv,is_call):\n",
    "        S = np.asarray(S, float); K=np.asarray(K, float); pv=np.asarray(pv, float)\n",
    "        is_call = np.asarray(is_call, bool)\n",
    "        S_eff = np.where(is_call, np.maximum(S - pv, 1e-12), S)\n",
    "        K_eff = np.where(is_call, K, K + pv)\n",
    "        return _bs_delta_vec(S_eff, K_eff, r, sig, T, is_call)\n",
    "\n",
    "    is_call_mask = df['cp_flag'].eq('C').values\n",
    "    df['delta_bs_plain']  = _bs_delta_vec( df['S0'].values, df['K'].values, df['r_eff'].values,\n",
    "                                           df[iv_col].values, df['T'].values, is_call_mask )\n",
    "    df['delta_bs_divadj'] = _bs_delta_div_vec( df['S0'].values, df['K'].values, df['r_eff'].values,\n",
    "                                               df[iv_col].values, df['T'].values, df['pv_div_ann'].values,\n",
    "                                               is_call_mask )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "DF_IN = opts2_ready.copy()     \n",
    "if 'cp_flag' not in DF_IN.columns:\n",
    "    DF_IN['cp_flag'] = 'C'\n",
    "opts_with_deltas = compute_deltas_on_df(DF_IN, iv_col=IV_COL, use_wrds=USE_WRDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f248dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = opts_with_deltas.copy()\n",
    "\n",
    "#numerics & safe fallbacks\n",
    "num_cols = ['S0','K','mid','price_am','delta_model','delta_bs_plain','delta_bs_divadj']\n",
    "for c in num_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "#days\n",
    "if 'days_to_expiry' in df.columns:\n",
    "    df['days'] = pd.to_numeric(df['days_to_expiry'], errors='coerce')\n",
    "elif 'T' in df.columns:\n",
    "    df['days'] = pd.to_numeric(df['T'], errors='coerce')*365.0\n",
    "else:\n",
    "    df['days'] = (pd.to_datetime(df['exdate']) - pd.to_datetime(df['date'])).dt.days\n",
    "\n",
    "#moneyness\n",
    "df['logm'] = np.log(df['S0'] / df['K'])\n",
    "\n",
    "#differences vs baselines\n",
    "df['d_model_vs_bs_plain']   = df['delta_model'] - df['delta_bs_plain']\n",
    "df['d_model_vs_bs_divadj']  = df['delta_model'] - df['delta_bs_divadj']\n",
    "\n",
    "#price basis (model American vs mid)\n",
    "df['price_basis_pct'] = (df['price_am'] - df['mid']) / df['mid']\n",
    "df['basis_bps']       = 1e4 * (df['price_am'] - df['mid']) / df['mid'].clip(lower=1e-6)\n",
    "\n",
    "mny_bins = [-0.4,-0.2,-0.1,-0.05,-0.02,0,0.02,0.05,0.1,0.2,0.4]\n",
    "day_bins = [0,30,60,91,122,152,182,273,365,9999]\n",
    "df['mny_bucket'] = pd.cut(df['logm'], mny_bins)\n",
    "df['T_bucket']   = pd.cut(df['days'], day_bins, right=True)\n",
    "\n",
    "def summarize_series(s: pd.Series) -> pd.Series:\n",
    "    s = pd.to_numeric(s, errors='coerce').dropna()\n",
    "    if s.empty:\n",
    "        return pd.Series({'N':0,'mean':np.nan,'MAE':np.nan,'RMSE':np.nan,'p10':np.nan,'p50':np.nan,'p90':np.nan})\n",
    "    return pd.Series({\n",
    "        'N':   s.size,\n",
    "        'mean': s.mean(),\n",
    "        'MAE':  s.abs().mean(),\n",
    "        'RMSE': np.sqrt((s**2).mean()),\n",
    "        'p10':  s.quantile(0.10),\n",
    "        'p50':  s.quantile(0.50),\n",
    "        'p90':  s.quantile(0.90),\n",
    "    })\n",
    "\n",
    "def corr_safe(a, b):\n",
    "    t = pd.DataFrame({'a':a, 'b':b}).dropna()\n",
    "    return t['a'].corr(t['b']) if len(t)>=2 else np.nan\n",
    "\n",
    "print(\"=== COVERAGE ===\")\n",
    "print(f\"N rows: {len(df):,}\")\n",
    "print(f\"delta_model coverage: {df['delta_model'].notna().mean():.2%}\")\n",
    "print(f\"days_to_expiry (min/median/max): {df['days'].min()} / {df['days'].median()} / {df['days'].max()}\")\n",
    "print()\n",
    "\n",
    "print(\"=== OVERALL (Δ_model − Δ_BS_plain) ===\")\n",
    "s1 = summarize_series(df['d_model_vs_bs_plain'])\n",
    "s1['corr(model, BS_plain)'] = corr_safe(df['delta_model'], df['delta_bs_plain'])\n",
    "display(pd.DataFrame([s1]))\n",
    "\n",
    "print(\"\\n=== OVERALL (Δ_model − Δ_BS_divadj) ===\")\n",
    "s2 = summarize_series(df['d_model_vs_bs_divadj'])\n",
    "s2['corr(model, BS_divadj)'] = corr_safe(df['delta_model'], df['delta_bs_divadj'])\n",
    "display(pd.DataFrame([s2]))\n",
    "\n",
    "print(\"\\n=== BY LOG-MONEYNESS (Δ_model − Δ_BS_divadj) ===\")\n",
    "g_mny = df.groupby('mny_bucket', observed=True)['d_model_vs_bs_divadj'].apply(summarize_series).unstack()\n",
    "display(g_mny)\n",
    "\n",
    "print(\"\\n=== BY DAYS TO EXPIRY (Δ_model − Δ_BS_divadj) ===\")\n",
    "g_T = df.groupby('T_bucket', observed=True)['d_model_vs_bs_divadj'].apply(summarize_series).unstack()\n",
    "display(g_T)\n",
    "\n",
    "print(\"\\n=== BASIS bps (all) ===\")\n",
    "display(df['basis_bps'].describe(percentiles=[.1,.5,.9,.99]).to_frame('basis_bps'))\n",
    "\n",
    "print(\"\\n=== BASIS bps (liquidity-filtered) ===\")\n",
    "if {'bid','ask'}.issubset(df.columns):\n",
    "    liq = (df['mid']>=0.5) & ((df['ask'] - df['bid']) <= 0.5)\n",
    "else:\n",
    "    liq = (df['mid']>=0.5)\n",
    "display(df.loc[liq, 'basis_bps'].describe(percentiles=[.1,.5,.9,.99]).to_frame('basis_bps_liq'))\n",
    "\n",
    "if 'has_div_future' in df.columns:\n",
    "    print(\"\\n=== BY has_div_future (ex-ante baseline, Δ_model − Δ_BS_divadj) ===\")\n",
    "    rows = []\n",
    "    for flag, sub in [('True', df[df['has_div_future']]), ('False', df[~df['has_div_future']])]:\n",
    "        s = summarize_series(sub['d_model_vs_bs_divadj'])\n",
    "        s['label'] = f\"has_div_future={flag}\"\n",
    "        rows.append(s)\n",
    "    display(pd.DataFrame(rows))\n",
    "\n",
    "if 'unknown_div_flag' in df.columns:\n",
    "    print(\"\\n=== Unknown dividends group (has_div_future=True & has_div_ann=False) ===\")\n",
    "    unk = df[df['unknown_div_flag']]\n",
    "    if len(unk):\n",
    "        u = summarize_series(unk['d_model_vs_bs_divadj'])\n",
    "        u['corr(model, BS_divadj)'] = corr_safe(unk['delta_model'], unk['delta_bs_divadj'])\n",
    "        display(pd.DataFrame([u]))\n",
    "    else:\n",
    "        print(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9762e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _num(x):\n",
    "    return pd.to_numeric(x, errors='coerce')\n",
    "\n",
    "def _asfloat_arr(x):\n",
    "    return np.asarray(pd.to_numeric(x, errors='coerce'), dtype=float)\n",
    "\n",
    "def _canon_ticker(s):\n",
    "    return re.sub(r'[.\\-]', '', str(s).upper()).strip()\n",
    "\n",
    "# ---------- RF: compounded period from daily FF ----------\n",
    "def add_rf_period_from_ff(df_opts: pd.DataFrame, ff_daily: pd.DataFrame,\n",
    "                          start_col='date', end_col=None, rf_col='RF',\n",
    "                          rf_is_percent=True, out_col='rf_compounded_period'):\n",
    "\n",
    "    df = df_opts.copy()\n",
    "    if end_col is None:\n",
    "        end_col = 'st_date' if 'st_date' in df.columns else 'exdate'\n",
    "\n",
    "    ff = ff_daily[['date', rf_col]].copy()\n",
    "    ff['date'] = pd.to_datetime(ff['date']).dt.normalize()\n",
    "    rf = _num(ff[rf_col])\n",
    "    if rf_is_percent:\n",
    "        rf = rf / 100.0\n",
    "    ff['log1p_rf'] = np.log1p(rf)\n",
    "    ff['cum_log']  = ff['log1p_rf'].cumsum()\n",
    "\n",
    "    df[start_col] = pd.to_datetime(df[start_col]).dt.normalize()\n",
    "    df[end_col]   = pd.to_datetime(df[end_col]).dt.normalize()\n",
    "\n",
    "    s = pd.merge_asof(df[[start_col]].sort_values(start_col),\n",
    "                      ff[['date','cum_log']].rename(columns={'date':'_d'}),\n",
    "                      left_on=start_col, right_on='_d', direction='backward')\n",
    "    e = pd.merge_asof(df[[end_col]].sort_values(end_col),\n",
    "                      ff[['date','cum_log']].rename(columns={'date':'_d'}),\n",
    "                      left_on=end_col, right_on='_d', direction='backward')\n",
    "    s = s['cum_log'].reindex(df.index).fillna(method='ffill').fillna(0.0).values\n",
    "    e = e['cum_log'].reindex(df.index).fillna(method='ffill').fillna(0.0).values\n",
    "\n",
    "    gross = np.exp(e - s)           #product of (1+RF)\n",
    "    df[out_col] = gross - 1.0       #period RF\n",
    "    return df\n",
    "\n",
    "#atm option choise 1 per stock\n",
    "def pick_atm_one_per_stock(df: pd.DataFrame, mny_low=0.8, mny_high=1.2) -> pd.DataFrame:\n",
    "    X = df.copy()\n",
    "    X['ticker_norm'] = X['ticker'].map(_canon_ticker)\n",
    "    X['date']   = pd.to_datetime(X['date']).dt.normalize()\n",
    "    X['exdate'] = pd.to_datetime(X['exdate']).dt.normalize()\n",
    "    X['mny']    = _num(X['S0']) / _num(X['K'])\n",
    "\n",
    "    atm = X[X['mny'].between(mny_low, mny_high)].copy()\n",
    "    atm['dist']      = (atm['mny'] - 1.0).abs()\n",
    "    atm['is_otm_pr'] = (atm['mny'] <= 1.0).astype(int)\n",
    "    atm = (atm.sort_values(['date','ticker_norm','dist','is_otm_pr'],\n",
    "                           ascending=[True, True, True, False])\n",
    "             .groupby(['date','ticker_norm'], as_index=False).head(1))\n",
    "    return atm.drop(columns=['dist','is_otm_pr'])\n",
    "\n",
    "#delta hedge returns\n",
    "def _pick_rf_period(df):\n",
    "    for c in ['rf_compounded_period','RF_period','rf_period','rf_monthly','rf']:\n",
    "        if c in df.columns:\n",
    "            return _asfloat_arr(df[c])\n",
    "    return np.zeros(len(df), dtype=float)\n",
    "\n",
    "def dh_return_eq3(S0, ST, K, Delta, C0, rf, eps=1e-12):\n",
    "    N = Delta*ST - np.maximum(ST - K, 0.0)\n",
    "    D = Delta*S0 - C0\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        r = N/D - 1.0 - rf\n",
    "    r[(~np.isfinite(r)) | (D<=eps)] = np.nan\n",
    "    return r, D\n",
    "\n",
    "def dh_return_tc_eq4_from_noTC(r_no_tc, rf, D, D_TC):\n",
    "    # (1 + r_no_tc + rf) * (D / D_TC) - 1 - rf\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        gross_tc = (1.0 + r_no_tc + rf) * (D / D_TC)\n",
    "        r_tc = gross_tc - 1.0 - rf\n",
    "    r_tc[(~np.isfinite(r_tc)) | (D_TC<=0)] = np.nan\n",
    "    return r_tc\n",
    "\n",
    "def compute_static_dh(df: pd.DataFrame,\n",
    "                      delta_col='delta_model',\n",
    "                      C_col='mid',          \n",
    "                      eff_frac=0.5,         \n",
    "                      side='sell',         \n",
    "                      eps=1e-12) -> pd.DataFrame:\n",
    "\n",
    "    out = df.copy()\n",
    "    out['date']   = pd.to_datetime(out['date']).dt.normalize()\n",
    "    out['exdate'] = pd.to_datetime(out['exdate']).dt.normalize()\n",
    "\n",
    "    S0, ST, K  = _asfloat_arr(out['S0']), _asfloat_arr(out['ST']), _asfloat_arr(out['K'])\n",
    "    Delta      = _asfloat_arr(out[delta_col])\n",
    "    C0         = _asfloat_arr(out[C_col])\n",
    "    rf         = _pick_rf_period(out)\n",
    "\n",
    "    r_no, D = dh_return_eq3(S0, ST, K, Delta, C0, rf, eps=eps)\n",
    "    out['r_dh_no_tc']  = r_no\n",
    "    out['denom_no_tc'] = D\n",
    "\n",
    "    if {'bid','ask'}.issubset(out.columns) and (eff_frac is not None):\n",
    "        half_spread = 0.5*np.maximum(_asfloat_arr(out['ask']) - _asfloat_arr(out['bid']), 0.0)\n",
    "        spread_eff  = float(eff_frac) * half_spread\n",
    "        C0_TC = (C0 - spread_eff) if side=='sell' else (C0 + spread_eff)\n",
    "        D_TC  = Delta*S0 - C0_TC\n",
    "        out['r_dh_tc']  = dh_return_tc_eq4_from_noTC(r_no, rf, D, D_TC)\n",
    "        out['denom_tc'] = D_TC\n",
    "\n",
    "    #doi weight\n",
    "    out['doi'] = _num(out['open_interest']) * _num(out['mid']) if 'open_interest' in out.columns else np.nan\n",
    "    return out\n",
    "\n",
    "def aggregate_monthly_returns(df_with_r: pd.DataFrame, use_tc=False) -> pd.DataFrame:\n",
    "    col = 'r_dh_tc' if use_tc else 'r_dh_no_tc'\n",
    "    g = df_with_r.loc[df_with_r[col].notna(), ['date', col, 'doi']].copy()\n",
    "    g['date'] = pd.to_datetime(g['date']).dt.normalize()\n",
    "    g['doi']  = _num(g['doi']).clip(lower=0)\n",
    "\n",
    "    ew  = g.groupby('date', observed=True)[col].mean().rename('EW')\n",
    "    sW  = g.groupby('date', observed=True)['doi'].sum().rename('sum_doi')\n",
    "    g   = g.join(sW, on='date')\n",
    "    g['w']  = np.where(g['sum_doi']>0, g['doi']/g['sum_doi'], np.nan)\n",
    "    g['wx'] = g['w']*g[col]\n",
    "    oiw = g.groupby('date', observed=True)['wx'].sum().rename('OIW')\n",
    "    N   = g.groupby('date', observed=True)[col].size().rename('N')\n",
    "    out = pd.concat([ew, oiw, N], axis=1).reset_index()\n",
    "    out['use_tc'] = bool(use_tc)\n",
    "    return out\n",
    "\n",
    "def _wstats_month(x, w=None):\n",
    "    x = _num(x).dropna()\n",
    "    if x.empty: return np.nan, np.nan\n",
    "    if w is None:\n",
    "        return float(x.mean()), float(x.std(ddof=0))\n",
    "    w = _num(w).reindex_like(x).fillna(0)\n",
    "    if (w<=0).all(): return np.nan, np.nan\n",
    "    wn = w / w.sum()\n",
    "    mu = float((x*wn).sum())\n",
    "    sd = float(np.sqrt(((x-mu)**2 * wn).sum()))\n",
    "    return mu, sd\n",
    "\n",
    "def _ts_avg_quantiles(groups, col):\n",
    "    mins=[]; q1s=[]; meds=[]; q3s=[]; maxs=[]\n",
    "    for _, g in groups:\n",
    "        x = _num(g[col]).dropna()\n",
    "        if x.empty: continue\n",
    "        mins.append(float(x.min()))\n",
    "        q1s.append(float(x.quantile(0.25)))\n",
    "        meds.append(float(x.quantile(0.50)))\n",
    "        q3s.append(float(x.quantile(0.75)))\n",
    "        maxs.append(float(x.max()))\n",
    "    if not mins: return [np.nan]*5\n",
    "    return [np.mean(mins), np.mean(q1s), np.mean(meds), np.mean(q3s), np.mean(maxs)]\n",
    "\n",
    "def build_table_I_like(opt_r: pd.DataFrame, use_tc=False, delta_col='delta_model'):\n",
    "    df = opt_r.copy()\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.normalize()\n",
    "\n",
    "    rcol = 'r_dh_tc' if use_tc else 'r_dh_no_tc'\n",
    "    df = df[df[rcol].notna()].copy()  # ίδιο sample για όλες τις γραμμές\n",
    "    df['opt_ret_pct'] = 100.0 * _num(df[rcol])\n",
    "\n",
    "    df['stock_ret_pct'] = 100.0 * (_num(df['ST'])/_num(df['S0']) - 1.0)\n",
    "    df['days']          = (pd.to_datetime(df['exdate']) - pd.to_datetime(df['date'])).dt.days\n",
    "    df['mny_S_over_K']  = _num(df['S0'])/_num(df['K'])\n",
    "    df['Delta_use']     = _num(df[delta_col]) if (delta_col in df.columns) else _num(df.get('delta', np.nan))\n",
    "\n",
    "    #greeks scaling\n",
    "    S = _num(df['S0'])\n",
    "    df['Gamma_Tbl'] = _num(df.get('gamma', np.nan)) * S    \n",
    "    df['Vega_Tbl']  = _num(df.get('vega',  np.nan)) / S\n",
    "    df['Theta_Tbl'] = _num(df.get('theta', np.nan)) / S\n",
    "\n",
    "    df['qspread_pct'] = 100.0 * (_num(df['ask']) - _num(df['bid'])) / _num(df['mid'])\n",
    "\n",
    "    #per-month stats\n",
    "    groups = df.groupby('date', observed=True)\n",
    "    ew_means = {lab: [] for lab in ['opt_ret_pct','stock_ret_pct','days','mny_S_over_K','Delta_use','Gamma_Tbl','Vega_Tbl','Theta_Tbl','qspread_pct']}\n",
    "    ew_sds   = {lab: [] for lab in ew_means}\n",
    "    oiw_means= {lab: [] for lab in ew_means}\n",
    "    oiw_sds  = {lab: [] for lab in ew_means}\n",
    "\n",
    "    for d, g in groups:\n",
    "        w = _num(g['doi']).clip(lower=0)\n",
    "        for lab in ew_means.keys():\n",
    "            mu_ew, sd_ew   = _wstats_month(g[lab], w=None)\n",
    "            mu_oiw, sd_oiw = _wstats_month(g[lab], w=w)\n",
    "            ew_means[lab].append(mu_ew); ew_sds[lab].append(sd_ew)\n",
    "            oiw_means[lab].append(mu_oiw); oiw_sds[lab].append(sd_oiw)\n",
    "\n",
    "    def ts_avg(series_dict):\n",
    "        return {k: np.nanmean(v) if len(v) else np.nan for k,v in series_dict.items()}\n",
    "\n",
    "    mean_OIW = ts_avg(oiw_means); mean_EW  = ts_avg(ew_means)\n",
    "    sd_OIW   = ts_avg(oiw_sds);   sd_EW    = ts_avg(ew_sds)\n",
    "\n",
    "    #unweighted monthly quantiles/min/max \n",
    "    Min,Q1,Q2,Q3,Max = {},{},{},{},{}\n",
    "    for lab in ew_means.keys():\n",
    "        m,q1,q2,q3,M = _ts_avg_quantiles(groups, lab)\n",
    "        Min[lab]=m; Q1[lab]=q1; Q2[lab]=q2; Q3[lab]=q3; Max[lab]=M\n",
    "\n",
    "    Count = np.mean(groups.size().values) if len(groups) else np.nan\n",
    "\n",
    "    def row(lab):\n",
    "        return [mean_OIW[lab], mean_EW[lab], sd_OIW[lab], sd_EW[lab],\n",
    "                Min[lab], Q1[lab], Q2[lab], Q3[lab], Max[lab], Count]\n",
    "\n",
    "    idx = ['Option return (%)','Stock return (%)','Days to maturity','Moneyness (S/K)',\n",
    "           'Delta','Gamma','Vega','Theta','Quoted spread (%)']\n",
    "    rows = [\n",
    "        row('opt_ret_pct'),\n",
    "        row('stock_ret_pct'),\n",
    "        row('days'),\n",
    "        row('mny_S_over_K'),\n",
    "        row('Delta_use'),\n",
    "        row('Gamma_Tbl'),\n",
    "        row('Vega_Tbl'),\n",
    "        row('Theta_Tbl'),\n",
    "        row('qspread_pct'),\n",
    "    ]\n",
    "    cols = pd.MultiIndex.from_tuples(\n",
    "        [('Mean','OIW'), ('Mean','EW'), ('Sd','OIW'), ('Sd','EW'),\n",
    "         ('','Min'),('','Q1'),('','Q2'),('','Q3'),('','Max'),('','Count')]\n",
    "    )\n",
    "    tbl = pd.DataFrame(rows, index=idx, columns=cols)\n",
    "    return tbl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c5ca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm = pick_atm_one_per_stock(opts_with_deltas, mny_low=0.8, mny_high=1.2)\n",
    "opt_r = compute_static_dh(atm, delta_col='delta_model', C_col='mid', eff_frac=0.5, side='sell')\n",
    "rets_no = aggregate_monthly_returns(opt_r, use_tc=False)\n",
    "rets_tc = aggregate_monthly_returns(opt_r, use_tc=True)\n",
    "\n",
    "table_no = build_table_I_like(opt_r, use_tc=False, delta_col='delta_model')\n",
    "table_tc = build_table_I_like(opt_r, use_tc=True,  delta_col='delta_model') \n",
    "\n",
    "print(table_no.round(2))\n",
    "print(table_tc.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba044bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "surf_df = pd.read_parquet('/content/drive/MyDrive/Dissertation/vol_filt_5_22.parquet') #dataset not available on github but information how to obtain it from wrds availabe on read.me file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee90a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "GRID_DAYS_DEFAULT   = [30, 60, 91, 122, 152, 182, 273, 365, 547, 730] \n",
    "GRID_DELTAS_DEFAULT = [d for d in range(-50, 51, 5) if d != 0] \n",
    "\n",
    "def _to_month_start(s) -> pd.Series:\n",
    "    s = pd.to_datetime(s)\n",
    "    return s.dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "def _nearest_index_vectorized(values: np.ndarray, grid: List[int]) -> np.ndarray:\n",
    "    g = np.asarray(grid, dtype=float)\n",
    "    v = np.asarray(values, dtype=float)\n",
    "    return np.abs(v[:, None] - g[None, :]).argmin(axis=1).astype(np.int64)\n",
    "\n",
    "def _auto_grid_deltas(surf_df: pd.DataFrame) -> List[int]:\n",
    "    try:\n",
    "        vals = (surf_df['delta'].round().astype(int)\n",
    "                .value_counts().nlargest(18).index.tolist())\n",
    "        vals = sorted(vals)\n",
    "        if len(vals) == 18:\n",
    "            return vals\n",
    "    except Exception:\n",
    "        pass\n",
    "    return GRID_DELTAS_DEFAULT\n",
    "\n",
    "#normalize and create the import dataset for the cnn\n",
    "@dataclass\n",
    "class GridScaler:\n",
    "    mu: np.ndarray  \n",
    "    sd: np.ndarray  \n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        return (X - self.mu) / np.where(self.sd==0, 1.0, self.sd)\n",
    "\n",
    "def build_cnn_dataset_inram(\n",
    "    surf_df: pd.DataFrame,\n",
    "    opt_r: pd.DataFrame,\n",
    "    grid_days: Optional[List[int]] = None,\n",
    "    grid_deltas: Optional[List[int]] = None,\n",
    "    dtype_X: np.dtype = np.float16,\n",
    "    add_controls: bool = True,\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, object]:\n",
    "    grid_days   = GRID_DAYS_DEFAULT if grid_days is None else grid_days\n",
    "    grid_deltas = _auto_grid_deltas(surf_df) if grid_deltas is None else grid_deltas\n",
    "\n",
    "    S = surf_df[['secid','date','month','days','delta','impl_volatility']].copy()\n",
    "    S['date']  = pd.to_datetime(S['date']).dt.normalize()\n",
    "    S['month'] = _to_month_start(S['month'])\n",
    "    S['secid'] = pd.to_numeric(S['secid'], errors='coerce').astype('Int64')\n",
    "\n",
    "    if not S['days'].isin(grid_days).all():\n",
    "        S['days'] = pd.to_numeric(S['days'], errors='coerce').astype(float).round().astype(int)\n",
    "    S = S[S['days'].isin(grid_days)]\n",
    "\n",
    "    P = opt_r.copy()\n",
    "    for c in ['secid','month','s0_date']:\n",
    "        if c not in P.columns:\n",
    "            raise KeyError(f\"Λείπει στήλη '{c}' από το opt_r.\")\n",
    "    P['month']   = _to_month_start(P['month'])\n",
    "    P['s0_date'] = pd.to_datetime(P['s0_date']).dt.normalize()\n",
    "    P['secid']   = pd.to_numeric(P['secid'], errors='coerce').astype('Int64')\n",
    "\n",
    "    if 'doi' not in P.columns and {'open_interest','mid'}.issubset(P.columns):\n",
    "        P['doi'] = pd.to_numeric(P['open_interest'], errors='coerce') * pd.to_numeric(P['mid'], errors='coerce')\n",
    "\n",
    "\n",
    "    P = (P.sort_values(['secid','month'])\n",
    "           .drop_duplicates(subset=['secid','month'], keep='first')\n",
    "           .reset_index(drop=True))\n",
    "\n",
    "    J = S.merge(P[['secid','month','s0_date']], on=['secid','month'], how='inner', copy=False)\n",
    "    J['dd']  = (J['date'] - J['s0_date']).dt.days.astype('int32')\n",
    "    #keep the closest day\n",
    "    J['ord'] = np.where(J['dd']>=0, J['dd'].astype('int64'), 10**9 + np.abs(J['dd']).astype('int64'))\n",
    "    idx_min  = J.groupby(['secid','month'])['ord'].idxmin()\n",
    "    chosen   = J.loc[idx_min, ['secid','month','date']]\n",
    "    S_use    = S.merge(chosen, on=['secid','month','date'], how='inner')\n",
    "    if verbose:\n",
    "        print(f\"Surface επιλογές: {len(S_use):,} rows από {len(P):,} (secid,month).\")\n",
    "\n",
    "    if not S_use['delta'].isin(grid_deltas).all():\n",
    "        jid = _nearest_index_vectorized(S_use['delta'].to_numpy(), grid_deltas)\n",
    "    else:\n",
    "        map_del = {int(d):j for j,d in enumerate(grid_deltas)}\n",
    "        jid = S_use['delta'].map(map_del).to_numpy(dtype=np.int64)\n",
    "\n",
    "    iid = _nearest_index_vectorized(S_use['days'].to_numpy(), grid_days)\n",
    "\n",
    "    keys = P[['secid','month']].copy()\n",
    "    keys['row_id'] = np.arange(len(keys), dtype=np.int64)\n",
    "    S_use = S_use.merge(keys, on=['secid','month'], how='left')\n",
    "    rid = S_use['row_id'].to_numpy(dtype=np.int64)\n",
    "\n",
    "    N = len(keys)\n",
    "    sums = np.zeros((N, len(grid_days), len(grid_deltas)), dtype=np.float32)\n",
    "    cnts = np.zeros_like(sums, dtype=np.uint16)\n",
    "    vv   = pd.to_numeric(S_use['impl_volatility'], errors='coerce').to_numpy(dtype=np.float32)\n",
    "\n",
    "    np.add.at(sums, (rid, iid, jid), vv)\n",
    "    np.add.at(cnts, (rid, iid, jid), 1)\n",
    "\n",
    "    A = sums / np.where(cnts>0, cnts, np.nan)  \n",
    "    valid_rows = np.isfinite(A).all(axis=(1,2))\n",
    "    if verbose:\n",
    "        bad = (~valid_rows).sum()\n",
    "        print(f\"Full surfaces: {valid_rows.sum():,} / {N:,}  (drop {bad:,} rows no full 10×18)\")\n",
    "\n",
    "    A = A[valid_rows]\n",
    "    keys = keys.loc[valid_rows].reset_index(drop=True)\n",
    "    P_use = P.merge(keys[['secid','month']], on=['secid','month'], how='inner')\n",
    "\n",
    "    #build the meta values for transfering\n",
    "    meta_cols_base = ['secid','month','s0_date','doi','r_dh_no_tc','r_dh_tc']\n",
    "    extra_cols = [c for c in ['mid','bid','ask','iv','delta','gamma','vega','theta','S0','K','T','rf_annual_t']\n",
    "                  if c in P_use.columns]\n",
    "    meta = P_use[meta_cols_base + extra_cols].reset_index(drop=True)\n",
    "\n",
    "    #surface based controls\n",
    "    if add_controls:\n",
    "        def _nearest_idx(vals, x):\n",
    "            arr = np.asarray(vals, float); return int(np.argmin(np.abs(arr - x)))\n",
    "        i30, i365 = _nearest_idx(grid_days,30), _nearest_idx(grid_days,365)\n",
    "        jc50, jp50, jp20 = _nearest_idx(grid_deltas,50), _nearest_idx(grid_deltas,-50), _nearest_idx(grid_deltas,-20)\n",
    "        A30c = A[:, i30, jc50]; A30p = A[:, i30, jp50]\n",
    "        iv_atm_30  = 0.5*(A30c + A30p)\n",
    "        iv_atm_365 = 0.5*(A[:, i365, jc50] + A[:, i365, jp50]) * 0.5 * 2  # (ίδιο αποτέλεσμα)\n",
    "        meta['IVolATM']   = iv_atm_30.astype(np.float32)\n",
    "        meta['CP_Spread'] = (A30c - A30p).astype(np.float32)\n",
    "        meta['IVSlope']   = (iv_atm_365 - iv_atm_30).astype(np.float32)\n",
    "        meta['IVolSkew']  = (A[:, i30, jp20] - A30c).astype(np.float32)\n",
    "\n",
    "    #xreate cnn input\n",
    "    X_raw = A.astype(dtype_X)[:, None, :, :]             \n",
    "    years = pd.to_datetime(meta['month']).dt.year\n",
    "    start_year      = int(years.min())\n",
    "    first_test_year = start_year + 7\n",
    "    train_mask_init = years.between(start_year, first_test_year-1)\n",
    "    if train_mask_init.sum() == 0:\n",
    "        raise RuntimeError(\"no 7 years window\")\n",
    "\n",
    "    mu = X_raw[train_mask_init].astype(np.float32).mean(axis=0, keepdims=False)\n",
    "    sd = X_raw[train_mask_init].astype(np.float32).std(axis=0, ddof=1, keepdims=False)\n",
    "    scaler = GridScaler(mu=mu, sd=np.where(sd==0, 1.0, sd).astype(np.float32))\n",
    "\n",
    "    # scaled X with float 32\n",
    "    Xz = scaler.transform(X_raw.astype(np.float32)).astype(np.float32)\n",
    "\n",
    "    #expanding splits\n",
    "    last_test_year = int(years.max())\n",
    "    splits: Dict[int, Dict[str, np.ndarray]] = {}\n",
    "    for y in range(first_test_year, last_test_year+1):\n",
    "        tr = years <= (y-1)\n",
    "        te = years == y\n",
    "        if te.sum() == 0 or tr.sum() == 0:\n",
    "            continue\n",
    "        splits[y] = dict(\n",
    "            train_idx = np.where(tr)[0],\n",
    "            test_idx  = np.where(te)[0],\n",
    "            weights_test = pd.to_numeric(meta.loc[te,'doi'], errors='coerce').astype(np.float32).to_numpy(),\n",
    "            secid_test   = meta.loc[te,'secid'].to_numpy(),\n",
    "            month_test   = pd.to_datetime(meta.loc[te,'month']).to_numpy('datetime64[M]')\n",
    "        )\n",
    "\n",
    "    return dict(\n",
    "        X_raw=X_raw, Xz=Xz, meta=meta, scaler=scaler,\n",
    "        grid_days=grid_days, grid_deltas=grid_deltas,\n",
    "        start_year=start_year, first_test_year=first_test_year, last_test_year=last_test_year,\n",
    "        splits=splits\n",
    "    )\n",
    "\n",
    "def export_year_npz(X: np.ndarray, y: np.ndarray, idx: np.ndarray, out_path: str):\n",
    "    Xb = X[idx].astype(np.float32, copy=False)\n",
    "    yb = y[idx].astype(np.float32, copy=False)\n",
    "    np.savez_compressed(out_path, X=Xb, y=yb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7870132",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = build_cnn_dataset_inram(surf_df, opt_r, dtype_X=np.float16, add_controls=True, verbose=True)\n",
    "\n",
    "Xz   = res['Xz']        \n",
    "meta = res['meta']      \n",
    "spl  = res['splits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d08d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "EXPORT_DIR = \"/content/drive/MyDrive/Dissertation/data\"  #colab directories\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "xz_path = os.path.join(EXPORT_DIR, \"Xz_float32.npy\")\n",
    "np.save(xz_path, Xz.astype(np.float32, copy=False))\n",
    "#parquet for minimal space \n",
    "meta_path = os.path.join(EXPORT_DIR, \"meta.parquet\")\n",
    "meta.to_parquet(meta_path, index=False)\n",
    "\n",
    "if 'scaler' in res:\n",
    "    sc_path = os.path.join(EXPORT_DIR, \"grid_scaler_init7y.npz\")\n",
    "    np.savez_compressed(sc_path, mu=res['scaler'].mu, sd=res['scaler'].sd)\n",
    "else:\n",
    "    sc_path = None\n",
    "\n",
    "splits_json = {}\n",
    "for y, pack in spl.items():\n",
    "    splits_json[str(y)] = {\n",
    "        'train_idx': np.asarray(pack['train_idx']).tolist(),\n",
    "        'test_idx' : np.asarray(pack['test_idx']).tolist()\n",
    "    }\n",
    "sp_path = os.path.join(EXPORT_DIR, \"splits.json\")\n",
    "with open(sp_path, \"w\") as f:\n",
    "    json.dump(splits_json, f)\n",
    "\n",
    "#we can remember what we saved\n",
    "manifest = {\n",
    "    \"created_at_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"Xz_shape\": tuple(map(int, Xz.shape)),\n",
    "    \"rows_meta\": int(len(meta)),\n",
    "    \"oos_years\": sorted(map(int, spl.keys())),\n",
    "    \"paths\": {\"Xz\": xz_path, \"meta\": meta_path, \"scaler\": sc_path, \"splits\": sp_path}\n",
    "}\n",
    "with open(os.path.join(EXPORT_DIR, \"manifest.json\"), \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a7e41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def ensure_month_doi(meta: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = meta.copy()\n",
    "    if 'month' in df:\n",
    "        m = pd.to_datetime(df['month'], errors='coerce')\n",
    "    elif 's0_date' in df:\n",
    "        m = pd.to_datetime(df['s0_date'], errors='coerce')\n",
    "    else:\n",
    "        raise KeyError(\"Need 'month' or 's0_date' to build calendar month.\")\n",
    "    df['month'] = m.dt.to_period('M').dt.to_timestamp()\n",
    "    #doi weight\n",
    "    if 'doi' not in df or df['doi'].isna().all():\n",
    "        if {'open_interest','mid'}.issubset(df.columns):\n",
    "            df['doi'] = pd.to_numeric(df['open_interest'], errors='coerce') * pd.to_numeric(df['mid'], errors='coerce')\n",
    "        else:\n",
    "            df['doi'] = 1.0\n",
    "    df['doi'] = pd.to_numeric(df['doi'], errors='coerce').fillna(0.0).clip(lower=0)\n",
    "    return df\n",
    "\n",
    "def load_splits(res=None, json_path=None):\n",
    "    if res is not None and 'splits' in res:\n",
    "        return res['splits']\n",
    "    if json_path:\n",
    "        import json\n",
    "        with open(json_path,'r') as f: sj = json.load(f)\n",
    "        return {int(y): {'train_idx': np.asarray(v['train_idx'], int),\n",
    "                         'test_idx' : np.asarray(v['test_idx'],  int)} for y,v in sj.items()}\n",
    "    raise ValueError(\"Pass res['splits'] or a path to splits.json\")\n",
    "\n",
    "#Pca espanding\n",
    "try:\n",
    "    from sklearn.decomposition import PCA as _SkPCA\n",
    "    _HAS_SK = True\n",
    "except Exception:\n",
    "    _HAS_SK = False\n",
    "\n",
    "class FittedPCA:\n",
    "    def __init__(self, mean_, comps_, evr_):\n",
    "        self.mean_ = mean_; self.components_ = comps_; self.evr_ = evr_\n",
    "\n",
    "def _fit_pca_np(X, k):\n",
    "    mu = X.mean(axis=0); Xc = X - mu\n",
    "    U,S,Vt = np.linalg.svd(Xc, full_matrices=False)\n",
    "    evr = (S**2)/(S**2).sum()\n",
    "    return FittedPCA(mu, Vt[:k,:], evr[:k])\n",
    "\n",
    "def _fit_pca(X, k):\n",
    "    if _HAS_SK:\n",
    "        p = _SkPCA(n_components=k, svd_solver='full', random_state=0).fit(X)\n",
    "        return FittedPCA(p.mean_.copy(), p.components_.copy(), p.explained_variance_ratio_.copy())\n",
    "    return _fit_pca_np(X, k)\n",
    "\n",
    "def _pca_scores(model, X): return (X - model.mean_) @ model.components_.T\n",
    "\n",
    "def compute_pcs_expanding(Xz, meta_df, splits, var_threshold=0.99, k_max=8, align_sign=True):\n",
    "    N = Xz.shape[0]; X = Xz.reshape(N,-1).astype(np.float32)\n",
    "    first_y = sorted(splits.keys())[0]; tr0 = splits[first_y]['train_idx']\n",
    "    # K from first train\n",
    "    pfull = _fit_pca(X[tr0], min(k_max, X.shape[1]))\n",
    "    K = int(np.searchsorted(np.cumsum(pfull.evr_), var_threshold) + 1)\n",
    "    K = max(1, min(K, k_max))\n",
    "    pref = _fit_pca(X[tr0], K); ref = pref.components_.copy()\n",
    "\n",
    "    scores = np.full((N,K), np.nan, np.float32); models={}\n",
    "    for y in sorted(splits.keys()):\n",
    "        tr, te = splits[y]['train_idx'], splits[y]['test_idx']\n",
    "        pj = _fit_pca(X[tr], K)\n",
    "        if align_sign:\n",
    "            for j in range(K):\n",
    "                if np.dot(pj.components_[j], ref[j]) < 0: pj.components_[j] *= -1.0\n",
    "        models[y] = pj\n",
    "        scores[tr,:] = _pca_scores(pj, X[tr])\n",
    "        scores[te,:] = _pca_scores(pj, X[te])\n",
    "\n",
    "    meta_pca = meta_df.copy()\n",
    "    for j in range(K): meta_pca[f'PC{j+1}'] = scores[:,j]\n",
    "    info = {'K': K, 'cumvar_first_train': float(np.cumsum(pref.evr_)[-1])}\n",
    "    return meta_pca, models, info\n",
    "\n",
    "#wls baseline\n",
    "def _standardize_train_test(Xtr, Xte):\n",
    "    mu = np.nanmean(Xtr, axis=0)\n",
    "    sd = np.nanstd(Xtr, axis=0, ddof=0)\n",
    "    sd[~np.isfinite(sd) | (sd==0)] = 1.0\n",
    "    return (Xtr-mu)/sd, (Xte-mu)/sd\n",
    "\n",
    "def _ols_wls(Xtr, ytr, wtr, Xte, ridge=1e-4):\n",
    "    Xtr_ = np.c_[np.ones(len(Xtr)), Xtr]; Xte_ = np.c_[np.ones(len(Xte)), Xte]\n",
    " \n",
    "    w = np.asarray(wtr, float); w[~np.isfinite(w) | (w<0)] = 0.0\n",
    "    if w.sum() <= 0:  #fallback EW\n",
    "        w = np.ones_like(w)\n",
    "    W = np.diag(w)\n",
    "    A = Xtr_.T @ W @ Xtr_ + ridge*np.eye(Xtr_.shape[1])\n",
    "    b = Xtr_.T @ W @ ytr\n",
    "    try:\n",
    "        beta = np.linalg.solve(A,b)\n",
    "    except np.linalg.LinAlgError:\n",
    "        beta = np.linalg.lstsq(A,b,rcond=None)[0]\n",
    "    return Xtr_@beta, Xte_@beta\n",
    "\n",
    "def run_pca_baseline(meta_pca, splits, target_col='r_dh_no_tc', weight_col='doi'):\n",
    "    pc_cols = [c for c in meta_pca.columns if c.startswith('PC')]\n",
    "    if not pc_cols:\n",
    "        raise RuntimeError(\"No PC columns found in meta_pca.\")\n",
    "    y_hat = np.full(len(meta_pca), np.nan, np.float32)\n",
    "    wmse_num = wmse_den = emse_num = emse_den = 0.0\n",
    "\n",
    "    for y in sorted(splits.keys()):\n",
    "        tr, te = splits[y]['train_idx'], splits[y]['test_idx']\n",
    "        df_tr = meta_pca.iloc[tr].copy()\n",
    "        df_te = meta_pca.iloc[te].copy()\n",
    "        \n",
    "        mask_tr = np.isfinite(df_tr[target_col].to_numpy(float))\n",
    "        for c in pc_cols: mask_tr &= np.isfinite(df_tr[c].to_numpy(float))\n",
    "        mask_tr &= (pd.to_numeric(df_tr[weight_col], errors='coerce').to_numpy(float) >= 0)\n",
    "        df_tr = df_tr.loc[mask_tr]\n",
    "        if df_tr.empty or len(df_te)==0:\n",
    "            continue\n",
    "\n",
    "        Xtr = df_tr[pc_cols].to_numpy(float)\n",
    "        ytr = df_tr[target_col].to_numpy(float)\n",
    "        wtr = pd.to_numeric(df_tr[weight_col], errors='coerce').to_numpy(float)\n",
    "\n",
    "        Xte = df_te[pc_cols].to_numpy(float)\n",
    "        Xtr, Xte = _standardize_train_test(Xtr, Xte)\n",
    "        \n",
    "        _, yhat_te = _ols_wls(Xtr, ytr, wtr, Xte, ridge=1e-4)\n",
    "        y_hat[te] = yhat_te\n",
    "\n",
    "        #metrics\n",
    "        yte = pd.to_numeric(df_te[target_col], errors='coerce').to_numpy(float)\n",
    "        wte = pd.to_numeric(df_te[weight_col], errors='coerce').to_numpy(float)\n",
    "        m = np.isfinite(yte) & np.isfinite(yhat_te)\n",
    "        wmse_num += np.nansum(wte[m]*(yhat_te[m]-yte[m])**2); wmse_den += np.nansum(wte[m])\n",
    "        emse_num += np.nansum((yhat_te[m]-yte[m])**2);       emse_den += np.sum(m)\n",
    "\n",
    "    metrics = {\n",
    "        'OOS_WMSE': float(wmse_num / wmse_den) if wmse_den>0 else np.nan,\n",
    "        'OOS_EMSE': float(emse_num / max(emse_den,1))\n",
    "    }\n",
    "    return y_hat, metrics\n",
    "\n",
    "#deciles\n",
    "def decile_sorts_safe(meta_df, pred, ret_col='r_dh_no_tc', w_col='doi', time_col='month'):\n",
    "    df = meta_df[[time_col, ret_col, w_col]].copy()\n",
    "    df['pred'] = np.asarray(pred, float)\n",
    "    df = df.dropna(subset=['pred', ret_col])\n",
    "    #ensure month dtype\n",
    "    df[time_col] = pd.to_datetime(df[time_col], errors='coerce').dt.to_period('M').dt.to_timestamp()\n",
    "    out = []\n",
    "    for mth, g in df.groupby(time_col, observed=True):\n",
    "        g = g.sort_values('pred')\n",
    "        #need at least 10 obs and at least 2 unique preds\n",
    "        if len(g) < 10 or g['pred'].nunique() < 2:\n",
    "            continue\n",
    "        g['dec'] = pd.qcut(g['pred'], 10, labels=False, duplicates='drop') + 1\n",
    "        ew = g.groupby('dec')[ret_col].mean()\n",
    "        w = pd.to_numeric(g[w_col], errors='coerce').clip(lower=0).fillna(0.0)\n",
    "        Wsum = w.groupby(g['dec']).sum().replace(0, np.nan)\n",
    "        oiw = (g[ret_col]*w).groupby(g['dec']).sum() / Wsum\n",
    "        if 1 in ew.index and 10 in ew.index:\n",
    "            out.append({'month': mth,\n",
    "                        'EW_LS': float(ew.loc[10]-ew.loc[1]),\n",
    "                        'OIW_LS': float(oiw.loc[10]-oiw.loc[1])})\n",
    "    if not out:\n",
    "        return pd.DataFrame(columns=['month','EW_LS','OIW_LS']), {\n",
    "            'months': 0, 'EW_LS_mean': np.nan, 'EW_LS_ann_sharpe': np.nan,\n",
    "            'OIW_LS_mean': np.nan, 'OIW_LS_ann_sharpe': np.nan\n",
    "        }\n",
    "    res = pd.DataFrame(out).sort_values('month')\n",
    "    summ = {\n",
    "        'months': int(len(res)),\n",
    "        'EW_LS_mean': float(res['EW_LS'].mean()),\n",
    "        'EW_LS_ann_sharpe': float(np.sqrt(12)*res['EW_LS'].mean()/ (res['EW_LS'].std(ddof=1)+1e-12)) if len(res)>1 else np.nan,\n",
    "        'OIW_LS_mean': float(res['OIW_LS'].mean()),\n",
    "        'OIW_LS_ann_sharpe': float(np.sqrt(12)*res['OIW_LS'].mean()/ (res['OIW_LS'].std(ddof=1)+1e-12)) if len(res)>1 else np.nan,\n",
    "    }\n",
    "    return res, summ\n",
    "\n",
    "\n",
    "meta_fix = ensure_month_doi(meta)\n",
    "splits = load_splits(res=res) \n",
    "\n",
    "meta_pca, pca_models, pinfo = compute_pcs_expanding(Xz, meta_fix, splits, var_threshold=0.99, k_max=8)\n",
    "print(f\"[PCA] K={pinfo['K']} | cumvar(first train)={pinfo['cumvar_first_train']:.3f}\")\n",
    "\n",
    "y_hat_pc, metrics = run_pca_baseline(meta_pca, splits, target_col='r_dh_no_tc', weight_col='doi')\n",
    "print(\"[Baseline] OOS WMSE:\", metrics['OOS_WMSE'], \"| OOS EW-MSE:\", metrics['OOS_EMSE'])\n",
    "\n",
    "deciles_ts, deciles_sum = decile_sorts_safe(meta_pca, y_hat_pc, ret_col='r_dh_no_tc', w_col='doi', time_col='month')\n",
    "print(\"[Deciles] summary:\", deciles_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bda28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Newey–West t-stat\n",
    "def newey_west_t(x, L=6):\n",
    "    x = pd.to_numeric(pd.Series(x).dropna(), errors='coerce').dropna().to_numpy(float)\n",
    "    n = len(x)\n",
    "    if n == 0:\n",
    "        return np.nan, np.nan, np.nan, 0\n",
    "    m = x.mean(); u = x - m\n",
    "    def acov(l): return np.dot(u[l:], u[:-l]) / n if l>0 else np.dot(u, u) / n\n",
    "    S = acov(0)\n",
    "    for l in range(1, min(L, n-1)+1):\n",
    "        w = 1.0 - l/(L+1.0)\n",
    "        S += 2.0 * w * acov(l)\n",
    "    se = np.sqrt(S / n); t = m / se if se>0 else np.nan\n",
    "    return float(m), float(se), float(t), n\n",
    "\n",
    "#Rank-IC (Spearman)\n",
    "def rank_ic_series(meta_df, pred, ret_col='r_dh_no_tc', time_col='month'):\n",
    "    df = meta_df[[time_col, ret_col]].copy()\n",
    "    df['pred'] = np.asarray(pred, float)\n",
    "    df = df.dropna(subset=[time_col, ret_col, 'pred'])\n",
    "    df[time_col] = pd.to_datetime(df[time_col], errors='coerce').dt.to_period('M').dt.to_timestamp()\n",
    "    out = []\n",
    "    for m, g in df.groupby(time_col, observed=True):\n",
    "        if len(g) < 3: \n",
    "            continue\n",
    "        rp = g['pred'].rank(method='average'); rr = g[ret_col].rank(method='average')\n",
    "        ic = np.corrcoef(rp, rr)[0,1]\n",
    "        if np.isfinite(ic): out.append({'month': m, 'IC': float(ic)})\n",
    "    ic_df = pd.DataFrame(out).sort_values('month')\n",
    "    mu, se, t, n = newey_west_t(ic_df['IC'], L=6) if len(ic_df) else (np.nan,)*4\n",
    "    return ic_df, {'mean_IC':mu,'NW_se':se,'NW_t':t,'n_months':n}\n",
    "\n",
    "#deciles long sort\n",
    "def decile_ts(meta_df, pred, ret_col='r_dh_no_tc', w_col='doi', time_col='month'):\n",
    "    df = meta_df[[time_col, ret_col, w_col]].copy()\n",
    "    df['pred'] = np.asarray(pred, float)\n",
    "    df = df.dropna(subset=[time_col, ret_col, 'pred'])\n",
    "    df[time_col] = pd.to_datetime(df[time_col], errors='coerce').dt.to_period('M').dt.to_timestamp()\n",
    "    rows = []\n",
    "    for m, g in df.groupby(time_col, observed=True):\n",
    "        g = g.sort_values('pred')\n",
    "        if len(g) < 10 or g['pred'].nunique() < 2: \n",
    "            continue\n",
    "        g['dec'] = pd.qcut(g['pred'], 10, labels=False, duplicates='drop') + 1\n",
    "        ew = g.groupby('dec')[ret_col].mean()\n",
    "        w  = pd.to_numeric(g[w_col], errors='coerce').clip(lower=0).fillna(0.0)\n",
    "        Wsum = w.groupby(g['dec']).sum().replace(0, np.nan)\n",
    "        oiw = (g[ret_col]*w).groupby(g['dec']).sum() / Wsum\n",
    "        if 1 in ew.index and 10 in ew.index:\n",
    "            rows.append({'month': m, 'EW_LS': float(ew.loc[10]-ew.loc[1]),\n",
    "                                  'OIW_LS': float(oiw.loc[10]-oiw.loc[1])})\n",
    "    ts = pd.DataFrame(rows).sort_values('month')\n",
    "    ew_mu, ew_se, ew_t, _   = newey_west_t(ts['EW_LS'],  L=6) if len(ts) else (np.nan,)*4\n",
    "    oiw_mu, oiw_se, oiw_t, _= newey_west_t(ts['OIW_LS'], L=6) if len(ts) else (np.nan,)*4\n",
    "    return ts, {'months':int(len(ts)),\n",
    "                'EW_LS_mean':ew_mu, 'EW_LS_seNW':ew_se, 'EW_LS_tNW':ew_t,\n",
    "                'OIW_LS_mean':oiw_mu,'OIW_LS_seNW':oiw_se,'OIW_LS_tNW':oiw_t}\n",
    "\n",
    "def decile_bars(meta_df, pred, ret_col='r_dh_no_tc', w_col='doi'):\n",
    "    df = meta_df[[ret_col, w_col]].copy()\n",
    "    df['pred'] = np.asarray(pred, float)\n",
    "    df = df.dropna(subset=['pred', ret_col])\n",
    "    if df['pred'].nunique() < 10:\n",
    "        return None, None\n",
    "    df['dec'] = pd.qcut(df['pred'], 10, labels=False, duplicates='drop') + 1\n",
    "    ew  = df.groupby('dec')[ret_col].mean()\n",
    "    w   = pd.to_numeric(df[w_col], errors='coerce').clip(lower=0).fillna(0.0)\n",
    "    Wsum= w.groupby(df['dec']).sum().replace(0, np.nan)\n",
    "    oiw = (df[ret_col]*w).groupby(df['dec']).sum() / Wsum\n",
    "    return ew, oiw\n",
    "\n",
    "#pca explained variance\n",
    "def pca_explained_variance_curve(Xz, splits, k_max=20):\n",
    "    N = Xz.shape[0]; X = Xz.reshape(N, -1).astype(np.float32)\n",
    "    first_y = sorted(splits.keys())[0]; tr0 = splits[first_y]['train_idx']\n",
    "    try:\n",
    "        from sklearn.decomposition import PCA as SkPCA\n",
    "        evr = SkPCA(n_components=min(k_max, X.shape[1]), svd_solver='full', random_state=0)\\\n",
    "                .fit(X[tr0]).explained_variance_ratio_\n",
    "    except Exception:\n",
    "        Xc = X[tr0] - X[tr0].mean(axis=0); _, S, _ = np.linalg.svd(Xc, full_matrices=False)\n",
    "        evr = (S**2)/(S**2).sum(); evr = evr[:k_max]\n",
    "    cum = np.cumsum(evr)\n",
    "    return evr, cum\n",
    "\n",
    "#plots\n",
    "def show_rank_ic_ts(ic_df):\n",
    "    if ic_df is None or len(ic_df)==0: \n",
    "        print(\"No IC data.\"); return\n",
    "    plt.figure()\n",
    "    s = ic_df.set_index('month')['IC']\n",
    "    plt.plot(s.index, s.values)\n",
    "    plt.axhline(0, linewidth=1)\n",
    "    plt.title(\"Monthly Rank-IC (Spearman) — PCA baseline\")\n",
    "    plt.ylabel(\"IC\"); plt.xlabel(\"Month\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def show_ls_ts(ls_ts):\n",
    "    if ls_ts is None or len(ls_ts)==0:\n",
    "        print(\"No LS time-series.\"); return\n",
    "    # EW\n",
    "    plt.figure()\n",
    "    s = ls_ts.set_index('month')['EW_LS']\n",
    "    plt.plot(s.index, s.values); plt.axhline(0, linewidth=1)\n",
    "    plt.title(\"D10–D1 (EW)\"); plt.ylabel(\"Return\"); plt.xlabel(\"Month\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "    # OIW\n",
    "    plt.figure()\n",
    "    s = ls_ts.set_index('month')['OIW_LS']\n",
    "    plt.plot(s.index, s.values); plt.axhline(0, linewidth=1)\n",
    "    plt.title(\"D10–D1 (OIW)\"); plt.ylabel(\"Return\"); plt.xlabel(\"Month\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def show_decile_bars(ew, oiw):\n",
    "    if ew is None:\n",
    "        print(\"Not enough unique predictions to form deciles.\"); return\n",
    "    d = ew.index.astype(int)\n",
    "    plt.figure()\n",
    "    plt.bar(d-0.15, ew.values, width=0.3, label=\"EW\")\n",
    "    if oiw is not None:\n",
    "        plt.bar(d+0.15, oiw.values, width=0.3, label=\"OIW\")\n",
    "    plt.xlabel(\"Decile (by prediction)\"); plt.ylabel(\"Mean return\"); plt.title(\"Average return by decile\")\n",
    "    plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "def show_evr(evr, cum):\n",
    "    ks = np.arange(1, len(evr)+1)\n",
    "    plt.figure()\n",
    "    plt.bar(ks, evr)\n",
    "    plt.plot(ks, cum, marker='o', linewidth=1.5)\n",
    "    plt.xlabel(\"Principal component\"); plt.ylabel(\"Share\"); plt.title(\"PCA explained variance (first OOS train)\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def show_pc_heatmaps(pca_models, splits, grid_days, grid_deltas, K_plot=3):\n",
    "    first_y = sorted(pca_models.keys())[0]\n",
    "    comps = pca_models[first_y].components_\n",
    "    H, W = len(grid_days), len(grid_deltas)\n",
    "    Kp = min(K_plot, comps.shape[0])\n",
    "    for j in range(Kp):\n",
    "        M = comps[j].reshape(H, W)\n",
    "        plt.figure()\n",
    "        im = plt.imshow(M, aspect='auto', origin='lower')\n",
    "        plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "        plt.yticks(ticks=np.arange(H), labels=[str(d) for d in grid_days])\n",
    "        plt.xticks(ticks=np.arange(W), labels=[str(d) for d in grid_deltas], rotation=90)\n",
    "        plt.xlabel(\"Delta bucket\"); plt.ylabel(\"Days to maturity\")\n",
    "        plt.title(f\"PC{j+1} loadings (first OOS year = {first_y})\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "ic_df, ic_summ = rank_ic_series(meta_pca, y_hat_pc, ret_col='r_dh_no_tc', time_col='month')\n",
    "print(\"[Rank-IC] mean_IC={:.4f}, NW_se={:.4f}, NW_t={:.2f}, months={}\".format(\n",
    "    ic_summ['mean_IC'], ic_summ['NW_se'], ic_summ['NW_t'], ic_summ['n_months']))\n",
    "show_rank_ic_ts(ic_df)\n",
    "\n",
    "ls_ts, ls_summ = decile_ts(meta_pca, y_hat_pc, ret_col='r_dh_no_tc', w_col='doi', time_col='month')\n",
    "print(\"[Deciles LS] months={months}, EW mean={EW_LS_mean:.4f} (t_NW={EW_LS_tNW:.2f}), \"\n",
    "      \"OIW mean={OIW_LS_mean:.4f} (t_NW={OIW_LS_tNW:.2f})\".format(**ls_summ))\n",
    "show_ls_ts(ls_ts)\n",
    "\n",
    "ew_bar, oiw_bar = decile_bars(meta_pca, y_hat_pc, ret_col='r_dh_no_tc', w_col='doi')\n",
    "show_decile_bars(ew_bar, oiw_bar)\n",
    "\n",
    "evr, cum = pca_explained_variance_curve(Xz, splits, k_max=12)\n",
    "show_evr(evr, cum)\n",
    "\n",
    "show_pc_heatmaps(pca_models, splits, res['grid_days'], res['grid_deltas'], K_plot=3)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
